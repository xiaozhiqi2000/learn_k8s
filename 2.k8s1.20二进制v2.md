[TOC]

# K8S 集群部署

[REFERENCE]<https://kubernetes.io/docs/home/>

## 环境介绍

节点部署
| 主机名 | IP | 配置 | Role | 备注 |
| ---- | ------ | ---- | ---- | ------- |
| docker-k8s01 | 192.168.200.8 | 2C4G | Master,Etcd | kube-apiserver、kube-controller-manager、kube-scheduler |
| docker-k8s02 | 192.168.200.9 | 2C4G | Master,Etcd | kube-apiserver、kube-controller-manager、kube-scheduler |
| docker-k8s03 | 192.168.200.10 | 2C4G | Master,Etcd | kubelet、kube-proxy |
| docker-k8s04 | 192.168.200.11 | 2C4G | Node,Etcd | kubelet、kube-proxy |
| docker-k8s05 | 192.168.200.12 | 2C4G | Node,Etcd | kubelet、kube-proxy |
| VIP | 192.168.200.88 |   |   | kube-apiserver VIP |

网络部署
|网络 | IP段 |
| ---- | ------ |
| 物理网络 | 192.168.200.0/24 |
| POD网络 | 10.244.0.0/16 |
| SERVICE网络 | 172.21.0.0/16 |


依赖组件
|  组件名称  |  组件版本  |
| ------------- | ----------- |
| Etcd |  |
| Docker | |
| Go | |
| CNI | |
| CSI | |
| Dashboard |  |
| Heapster | |
| Cluster | |
| kube-dns | |
| Influxdb | |
| Grafana | |
| Kibana | |
| cAdvisor | |
| Fluentd | |
| Elasticsearch | |
| go-oidc | |
| calico | |
| crictl | |
| CoreDNS | |
| event-exporter | |
| metrics-server | |
| ingress-gce | |
| ingress-nginx | |
| ip-masq-agent | |
| hcsshim | |

## 环境初始化（所有节点)
```shell
# 配置Yum源
rm -f /etc/yum.repos.d/* && wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo && wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo && yum clean all && yum makecache fast

# 升级软件包,Centos7需要升级，Centos8可以按需升级系统
yum update -y --exclude=kernel* && reboot

# 主机名解析
cat > /etc/hosts <<EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.200.8 docker-k8s01 node1
192.168.200.9 docker-k8s02 node2
192.168.200.10 docker-k8s03 node3
192.168.200.11 docker-k8s04 node4
192.168.200.12 docker-k8s05 node5
EOF

# 密钥互通node1分发
ssh-keygen -P "" -f /root/.ssh/id_rsa
cat /root/.ssh/id_rsa.pub > /root/.ssh/authorized_keys
for i in {2..5};do scp -r .ssh node$i:~/ ;done

# 关闭防火墙
systemctl disable firewalld;systemctl stop firewalld

# 禁用SELinux，允许容器访问宿主机的文件系统
setenforce 0
sed -i 's@SELINUX=.*@SELINUX=disabled@' /etc/selinux/config

# 安装一些依赖常用软件
yum intall -y openssl unzip tree lrzsz net-tools chrony ntpdate

# 同步时间
systemctl enable chronyd --now
ntpdate ntp1.aliyun.com

# 关闭SWAP
swapoff -a && sysctl -w vm.swappiness=0 && echo "vm.swappiness = 0">> /etc/sysctl.conf  && sysctl -p
sed -ri 's/.*swap.*/#&/' /etc/fstab

# 文件描述符优化
cp -p /etc/security/limits.conf /etc/security/limits.conf.bak
sed -i  '/# End of file/i* soft nofile 65536' /etc/security/limits.conf
sed -i  '/# End of file/i* hard nofile 65536' /etc/security/limits.conf
sed -i  '/# End of file/i* soft nproc 65535' /etc/security/limits.conf
sed -i  '/# End of file/i* hard nproc 65535\n' /etc/security/limits.conf
```

## 升级内核（所有节点)
```shell
# 升级内核,安装长期支持版本，建议升级 4.19+，可以从http://193.49.22.109/elrepo/找
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm
yum --enablerepo=elrepo-kernel install kernel-lt -y

# 4.19 内核
wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm
yum install -y kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm

grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg

# 开启 User namespaces
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
grep user_namespace /boot/grub2/grub.cfg

# 查看版本
grubby --default-kernel

# 重启
reboot

# 查看内核版本
uname -a
Linux docker-k8s01 4.4.118-1.el7.elrepo.x86_64 #1 SMP Sun Feb 25 19:10:14 EST 2018 x86_64 x86_64 x86_64 GNU/Linux

# 安装 ipvs 软件包
yum install -y ipvsadm ipset sysstat conntrack libseccomp net-tools

# 加载ipvs模块,在内核 4.19+版本nf_contrack_ipv4已经改为nf_contrack，4.18以下使用 nf_contrack_ipv4即可
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack

# 加入以下内容,在内核 4.19+版本nf_contrack_ipv4已经改为nf_contrack，4.18以下使用 nf_contrack_ipv4即可
cat > /etc/modules-load.d/ipvs.conf <<EOF
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
# 开启这个服务
systemctl enable --now systemd-modules-load.service

# 加载br_netfilter iptables 模块
cat <<EOF >  /etc/sysctl.d/k8s.conf
br_netfilter
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory = 1
vm.panic_on_oom = 0
fs.inotify.max_user_watchs = 89100
fs.file-max = 52706963
fs.nr_open = 52706963
net.netfilter.nf_conntrack_max = 2310720
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_sysncookies = 1
net.ipv4.tcp_max_sys_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_sys_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF

# 重新加载
sysctl --system

# 查看是否加载上ipvs模块
lsmod | grep --color=auto -e ip_vs -e nf_conntrack

```

## 安装 Docker（所有节点)
[Docker官网安装文档](https://docs.docker.com/engine/install/)
[Docker镜像国内](https://kubernetes.feisky.xyz/appendix/mirrors)

```shell
# 下载阿里的docker源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo

# 安装 docker 19.03.15
yum -y install docker-ce-19.03.15-3.el7 docker-ce-cli-19.03.15-3.el7 containerd.io-1.4.3-3.1.el7 

# 配置docker目录
mkdir -p /data/docker /etc/docker /etc/systemd/system/docker.service.d

# 配置国内镜像加速
cat > /etc/docker/daemon.json << EOF
{
  "data-root": "/data/docker",
  "storage-driver": "overlay2",
  "registry-mirrors": [
      "https://0mmxqss3.mirror.aliyuncs.com",
      "https://docker.mirrors.ustc.edu.cn",
      "https://registry.docker-cn.com"
  ],
  "insecure-registries": ["registry.cn-hangzhou.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "live-restore": true,
  "log-driver": "json-file",
  "log-level": "warn",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
EOF

# 启动dcoker
systemctl start docker && systemctl enable docker --now && systemctl status docker
```

## 安装HA(master节点)

> haproxy + keepalived 也可以 nginx + keepalived 四层代理

### 1. haproxy

```shell
# 安装 haproxy 依赖
yum install -y haproxy keepalived libnl3-devel ipset-devel

# 配置 haproxy，注意修改成自己的IP
cat > /etc/haproxy/haproxy.cfg <<EOF
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /var/run/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master
        bind 0.0.0.0:8443
        mode tcp
        option tcplog
        balance source
        server s1 192.168.200.8:6443  check inter 10000 fall 2 rise 2 weight 1
        server s2 192.168.200.9:6443  check inter 10000 fall 2 rise 2 weight 1
        server s3 192.168.200.10:6443  check inter 10000 fall 2 rise 2 weight 1
EOF

# 分发其他节点
for node in node2 node3;do scp /etc/haproxy/haproxy.cfg $node:/etc/haproxy/;done

# 启动 haproxy
for node in node1 node2 node3;do ssh ${node} "systemctl start haproxy && systemctl enable haproxy --now";done

# 查看下服务情况
systemctl status haproxy
```

### 2. keepalived
```shell
# 配置 keepalived，注意修改成自己的IP和网卡名称
# master1 配置

cat > /etc/keepalived/keepalived.conf <<EOF
global_defs {
    script_user root 
    enable_script_security
    router_id lb-master
}

vrrp_script check-haproxy {
    script "/bin/bash -c 'if [[ $(netstat -nlp | grep 8443) ]]; then exit 0; else exit 1; fi'"
    interval 2
    weight -50
}

vrrp_instance VI-kube-master {
    state MASTER
    priority 120
    dont_track_primary
    interface ens33
    virtual_router_id 120
    advert_int 3
    track_script {
        check-haproxy
    }
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.200.88
    }
}

# notify "/container/service/keepalived/assets/notify.sh"
EOF

# 分发 keepalived
for node in node2 node3;do scp /etc/keepalived/keepalived.conf ${node}:/etc/keepalived/;done

# master2 修改
sed -i -e 's@state MASTER@state BACKUP@' -e 's@priority 120@priority 90@' /etc/keepalived/keepalived.conf

# master3 修改
sed -i -e 's@state MASTER@state BACKUP@' -e 's@priority 120@priority 100@' /etc/keepalived/keepalived.conf


# 启动 keepalived
for node in node1 node2 node3;do ssh ${node} "systemctl start keepalived && systemctl enable keepalived --now";done


# 查看下服务情况
systemctl status keepalived
```

## Etcd 集群部署（node01）

### 1. 安装 CFSSL 证书工具
```shell
wget -O /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 
wget -O /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget -O /usr/local/bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

chmod +x  /usr/local/bin/cfssl*
```

### 2. 准备相关目录
```shell
mkdir -p /root/k8s/etcd/{conf,ssl,bin,systemd}

mkdir -p /root/packages


# 创建相关用户及目录(在k8s-master1、k8s-master2、k8s-master3上都执行)
useradd -s -r /sbin/nologin etcd
mkdir -pv /var/lib/etcd
chown -R etcd.etcd /var/lib/etcd /etc/etcd/pki

# 创建etcd所需的目录
for node in node1 node2 node3;do ssh ${node} \
  "useradd -s /sbin/nologin etcd && \
  mkdir -p /etc/etcd/ssl /var/lib/etcd && \
  chown -R etcd.etcd /var/lib/etcd /etc/etcd/ssl";done
```

### 3. ca-config 配置文件

> 创建证书配置文件，配置通用，可在配置文件中定义多个profiles，在创建证书时使用指定的profiles，server auth为服务端证书，client auth表示客户端证书，即有server auth又有client auth表示双向认证证书。可使用cfssl print-defaults config生成默认配置


```shell
cat > /root/k8s/etcd/ssl/ca-config.json <<EOF
{
    "signing": {
        "default": {
            "expiry": "876000h"
        },
        "profiles": {
            "server": {
                "expiry": "876000h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth"
                ]
            },
            "client": {
                "expiry": "876000h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "876000h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF
```

### 4. etcd-ca  证书签名请求
```shell
cat > /root/k8s/etcd/ssl/etcd-ca-csr.json <<EOF
{
  "CN": "etcd-ca",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
    }
  ],
  "ca": {
    "expiry": "876000h"
  }
}
EOF
```

### 5. etcd 证书签名请求
```shell
cat > /root/k8s/etcd/ssl/etcd-csr.json <<EOF
{
    "CN": "etcd",
    "hosts": [
        "127.0.0.1",
        "localhost",
        "192.168.200.8",
        "192.168.200.9",
        "192.168.200.10"
    ],
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [{
        "C": "CN",
        "ST": "GuangDong",
        "L": "ShenZhen"
    }]
}
EOF
```

### 6. etcd-peer  证书签名请求

```shell
cat > /root/k8s/etcd/ssl/etcd-peer-csr.json <<EOF
{
    "CN": "etcd-peer",
    "hosts": [
        "127.0.0.1",
        "localhost",
        "192.168.200.8",
        "192.168.200.9",
        "192.168.200.10"
    ],
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [{
         "C": "CN",
         "ST": "GuangDong",
         "L": "ShenZhen"
    }]
}
EOF

```

### 7. 生成etcd证书

```shell
cd /root/k8s/etcd/ssl/
cfssl gencert --initca=true etcd-ca-csr.json | cfssljson --bare etcd-ca

cfssl gencert --ca etcd-ca.pem --ca-key etcd-ca-key.pem --config ca-config.json --profile peer etcd-csr.json | cfssljson --bare etcd

cfssl gencert --ca etcd-ca.pem --ca-key etcd-ca-key.pem --config ca-config.json --profile peer etcd-peer-csr.json | cfssljson --bare etcd-peer

for node in node1 node2 node3;do scp -r /root/k8s/etcd/ssl/etcd*.pem $node:/etc/etcd/ssl/;done
```

### 8.下载 etcd
```shell
cd
wget -O /root/packages/etcd-v3.4.13-linux-amd64.tar.gz https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz
tar xf /root/packages/etcd-v3.4.13-linux-amd64.tar.gz -C /root/packages/
cp -f /root/packages/etcd-v3.4.13-linux-amd64/etcd* /root/k8s/etcd/bin/
```

### 9. etcd 配置文件

> 创建一个新集群必须为new, 集群通信token，三个节点设置一样

```shell
cat > /root/k8s/etcd/conf/etcd.conf <<EOF
#[Member]
ETCD_NAME="etcd01"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.200.8:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.200.8:2379,http://127.0.0.1:2379"

#[Clustering]
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.200.8:2379"
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.200.8:2380"
ETCD_INITIAL_CLUSTER="etcd01=https://192.168.200.8:2380,etcd02=https://192.168.200.9:2380,etcd03=https://192.168.200.10:2380"
ETCD_INITIAL_CLUSTER_STATE="new"		
ETCD_INITIAL_CLUSTER_TOKEN="yZAyShN8Z+Harw=="	

#[Security]
ETCD_CLIENT_CERT_AUTH="true"
ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
ETCD_CERT_FILE="/etc/etcd/ssl/etcd.pem"
ETCD_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
ETCD_PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
ETCD_PEER_CERT_FILE="/etc/etcd/ssl/etcd-peer.pem"
ETCD_PEER_KEY_FILE="/etc/etcd/ssl/etcd-peer-key.pem"
EOF
```

### 10. etcd 启动文件

```shell
cat > /root/k8s/etcd/systemd/etcd.service <<EOF
[Unit]
Description=Etcd Server
Documentation=https://etcd.io/docs/
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=/etc/etcd/etcd.conf
WorkingDirectory=/var/lib/etcd
ExecStartPre=/usr/bin/chown -R etcd.etcd /var/lib/etcd /etc/etcd/ssl
ExecStart=/usr/local/bin/etcd
LimitNOFILE=65536
User=etcd
Group=etcd
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
```

### 11. 下发文件
```shell
# 分发etcd命令、etcd.conf、etcd.service
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/bin/ ${node}:/usr/local/bin/;done
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/conf/etcd.conf ${node}:/etc/etcd/etcd.conf;done
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/systemd/etcd.service ${node}:/lib/systemd/system/etcd.service;done

# node2 修改对应IP
sed -i -e 's@192.168.200.8@192.168.200.9@g' -e 's@etcd01@etcd02@' -e 's@etcd02=https://192.168.200.9:2380@etcd01=https://192.168.200.8:2380@' /etc/etcd/etcd.conf

# node3 修改对应IP
sed -i -e 's@192.168.200.8@192.168.200.10@g' -e 's@etcd01@etcd03@' -e 's@etcd03=https://192.168.200.10:2380@etcd01=https://192.168.200.8:2380@' /etc/etcd/etcd.conf
```

### 12. 启动etcd集群
```shell
# 启动etcd集群,由于集群需要保证quorum机制，因此至少有两个节点启动服务，集群初始化启动成功
systemctl daemon-reload && systemctl enable etcd --now

# 检查集群健康,node1,node2,node3,随便1个节点执行
etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem\
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 endpoint health
 
etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem\
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 member list
  
etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem  \
  --cert=/etc/etcd/ssl/etcd.pem   \
  --key=/etc/etcd/ssl/etcd-key.pem   \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 endpoint status

```

## Kubernetes 组件部署
### 1. 准备CA证书

[REFERENCE](https://kubernetes.io/zh/docs/setup/best-practices/certificates/)

#### 1.1. 准备相关目录
```shell
# 创建ssl文件夹
mkdir -p /root/k8s/ssl/ && cd /root/k8s/ssl/

for node in node1 node2 node3 node4 node5;do ssh ${node} "mkdir -p /etc/kubernetes/pki/ /var/log/kubernetes /var/lib/kubelet /var/lib/kube-proxy";done
```

#### 1.2. ca-config CA配置文件
```shell
# 和 etcd 的 ca-config.json 一样即可
cp /root/k8s/etcd/ssl/ca-config.json /root/k8s/ssl/ca-config.json
```

#### 1.3. ca 证书签名请求
```shell
cat > /root/k8s/ssl/ca-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
    }
  ],
  "ca": {
    "expiry": "876000h"
  }
}
EOF
```

#### 1.4. pront-proxy-ca 证书签名请求

```shell
cat > /root/k8s/ssl/front-proxy-ca-csr.json <<EOF
{
    "CN": "front-proxy-ca",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen"
        }
    ],
    "ca": {
        "expiry": "876000h"
    }
}
EOF
```

#### 1.5. 生成CA证书和CA key

```shell
cd /root/k8s/ssl && \
cfssl gencert --initca=true ca-csr.json | cfssljson --bare ca && \
cfssl gencert --initca=true front-proxy-ca-csr.json | cfssljson --bare front-proxy-ca

# 分发到 Master 节点
for node in node1 node2 node3;do scp -r /root/k8s/ssl/ca*.pem /root/k8s/ssl/front-proxy-ca*.pem $node:/etc/kubernetes/pki/;done && cd
```

### 2. 下载 kubernetes
```shell
# 目录准备
mkdir -p /root/k8s/{master,node}/{conf,bin,systemd}

# 下载k8s 1.20.1二进制包
wget -O /root/packages/kubernetes-server-linux-amd64.tar.gz https://dl.k8s.io/v1.20.1/kubernetes-server-linux-amd64.tar.gz

# 解压
cd /root/packages && \
tar xf kubernetes-server-linux-amd64.tar.gz && \
tar xf kubernetes/kubernetes-src.tar.gz -C kubernetes/

# 根据master，node进行归类
cp -f kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /root/k8s/master/bin/

cp -f kubernetes/server/bin/{kubelet,kube-proxy} /root/k8s/node/bin/

# 下发master二进制文件
for node in node1 node2 node3;do rsync -avP /root/k8s/master/bin/ ${node}:/usr/local/bin/;done

# 下发worker二进制文件
for node in node1 node2 node3 node4 node5;do rsync -avP /root/k8s/node/bin/ ${node}:/usr/local/bin/;done
```

### 3. 部署kube-apiserver 

#### 3.1. kube-apiserver 证书签名请求

> 192.168.200.88 是kube-apiserver 高可用负载均衡IP，172.21.0.1 是kubernetes集群service第一个IP地址，即kubernetes集群地址

```shell
cat > /root/k8s/ssl/kube-apiserver-csr.json <<EOF
{
  "CN": "kube-apiserver",
  "hosts": [
    "127.0.0.1",
    "192.168.200.88",
    "172.21.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
    }
  ]
}
EOF
```

#### 3.2. apiserver-etcd 证书签名请求

> apiserver 与 etcd 通信的客户端证书密钥

```shell
cat > /root/k8s/ssl/kube-apiserver-etcd-client-csr.json <<EOF
{
    "CN": "kube-apiserver-etcd-client",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen",
            "O": "system:masters"
        }
    ]
}
EOF
```

#### 3.3. apiserver-kubelet 证书签名请求

> kubelet 连接 apiserver的客户端证书

```shell
cat > /root/k8s/ssl/kube-apiserver-kubelet-client-csr.json <<EOF
{
    "CN": "kube-apiserver-kubelet-client",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen",
            "O": "system:masters"
        }
    ]
}
EOF
```

#### 3.4. front-proxy 证书签名请求

> api 的客户端证书聚合 https://kubernetes.io/zh/docs/tasks/extend-kubernetes/setup-extension-api-server/ ，用于metrices
>
> front-proxy-client证书csr请求文件，CN默认应该与kube-apiserver的--requestheader-allowed-names值相同

```shell
cat > /root/k8s/ssl/front-proxy-client-csr.json <<EOF
{
    "CN": "front-proxy-client",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen"
        }
    ]
}
EOF
```

#### 3.5. 生成sa使用的私钥和公钥

> serviceaccount 所使用的私钥和公钥

```shell
openssl genrsa -out /root/k8s/ssl/sa.key 2048
openssl rsa -in sa.key -pubout -out /root/k8s/ssl/sa.pub
```

#### 3.6. 生成证书

```shell
# kube-apiserver
cd /root/k8s/ssl/ && \
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile server kube-apiserver-csr.json | cfssljson --bare kube-apiserver

# kube-apiserver-etcd-client
cfssl gencert -ca /root/k8s/etcd/ssl/etcd-ca.pem -ca-key /root/k8s/etcd/ssl/etcd-ca-key.pem \
-config ca-config.json -profile client kube-apiserver-etcd-client-csr.json | cfssljson -bare kube-apiserver-etcd-client

# kube-apiserver-kubelet-client
cfssl gencert -ca ca.pem -ca-key ca-key.pem -config ca-config.json -profile client kube-apiserver-kubelet-client-csr.json | cfssljson -bare kube-apiserver-kubelet-client

# front-proxy-client
cfssl gencert -ca front-proxy-ca.pem -ca-key front-proxy-ca-key.pem -config ca-config.json -profile client front-proxy-client-csr.json | cfssljson -bare front-proxy-client

# 分发证书
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-apiserver*.pem /root/k8s/ssl/front-proxy-client*.pem /root/k8s/ssl/sa* $node:/etc/kubernetes/pki/;done
```

#### 3.7. kube-apiserver 配置文件

[启动参数](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/)

> 如果kebelet 不是 tls bootstrap自动颁发证书 需要制定 --token-auth-file=/etc/kubernetes/pki/token.csv

```shell
cat > /root/k8s/master/conf/kube-apiserver.conf <<EOF
KUBE_API_ARGS="--bind-address=0.0.0.0 \
--secure-port=6443 \
--advertise-address=192.168.200.8 \
--service-cluster-ip-range=172.21.0.0/16 \
--service-node-port-range=30000-32767 \
--etcd-servers=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 \
--etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \
--etcd-certfile=/etc/kubernetes/pki/kube-apiserver-etcd-client.pem \
--etcd-keyfile=/etc/kubernetes/pki/kube-apiserver-etcd-client-key.pem \
--anonymous-auth=false \
--client-ca-file=/etc/kubernetes/pki/ca.pem \
--kubelet-client-certificate=/etc/kubernetes/pki/kube-apiserver-kubelet-client.pem \
--kubelet-client-key=/etc/kubernetes/pki/kube-apiserver-kubelet-client-key.pem \
--kubelet-preferred-address-types=InternalIP,ExternnalIP,Hostname \
--tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem \
--tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem \
--service-account-key-file=/etc/kubernetes/pki/sa.pub \
--service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
--service-account-issuer=https://kubernetes.default.svc.cluster.local \
--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota \
--authorization-mode=RBAC,Node \
--enable-bootstrap-token-auth=true \
--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \
--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
--requestheader-allowed-names=front-proxy-client \
--requestheader-group-headers=X-Remote-Group \
--requestheader-extra-headers-prefix=X-Remote-Extra- \
--requestheader-username-headers=X-Remote-User \
--allow-privileged=true \
--enable-swagger-ui=true \
--logtostderr=false \
--alsologtostderr=true \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
```

#### 3.8. kube-apiserver 启动文件

> 如果生成了 token，需要指定 --token-auth-file=/etc/kubernetes/pki/token.csv

```shell
cat > /root/k8s/master/systemd/kube-apiserver.service <<EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
EnvironmentFile=/etc/kubernetes/kube-apiserver.conf
ExecStart=/usr/local/bin/kube-apiserver \$KUBE_API_ARGS
LimitNOFILE=65536
Restart=on-failure
RestartSec=5
Type=notify
[Install]
WantedBy=multi-user.target
EOF
```

#### 3.9. 分发文件
```shell
for node in node1 node2 node3;do scp /root/k8s/master/conf/kube-apiserver.conf $node:/etc/kubernetes/kube-apiserver.conf;done
for node in node1 node2 node3;do scp /root/k8s/master/systemd/kube-apiserver.service $node:/lib/systemd/system/kube-apiserver.service;done

# node2修改IP
sed -i -e 's@192.168.200.8@192.168.200.9@g' -e 's@192.168.200.9:2379@192.168.200.8:2379@' /etc/kubernetes/kube-apiserver.conf

# node3修改IP
sed -i -e 's@192.168.200.8@192.168.200.10@g' -e 's@192.168.200.10:2379@192.168.200.8:2379@' /etc/kubernetes/kube-apiserver.conf
```

#### 3.10. 启动 kube-apiserver
```shell
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-apiserver --now";done

# 查看是否正常
systemctl status kube-apiserver
journal -u kube-apiserver

```

### 4. 部署 kubectl

#### 4.1. kubernetes-admin 证书签名请求

> admin证书csr请求文件，该证书用于与kube-apiserver做权限验证管理k8s集群使用，CN部分为用户名，可以随意，"names"节点下的"O"部分必须为"system:masters","O"对应clusterrolebinding的cluster-admin中subject部分kind为Group的组名，表示将CN对应的用户名加入到该组，从而具有管理员权限

```shell
cat > /root/k8s/ssl/admin-csr.json <<EOF
{
    "CN": "kubernetes-admin",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen",
            "O": "system:masters"
        }
    ]
}
EOF
```

#### 4.2. 生成 kubectl 证书
```shell
cd /root/k8s/ssl/ && \
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client admin-csr.json | cfssljson --bare kubernetes-admin

# 分发证书
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kubernetes-admin*.pem $node:/etc/kubernetes/pki/;done
```

#### 4.3. 生成kubernetes-admin kubeconfig

[具体参考](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#config)

>生成集群管理员admin kubeconfig配置文件供kubectl调用，kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书，自动生成在 ~/.kube/config
```shell
# 设置一个集群叫 kubernetes
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443
 
# 设置一个用户叫kubernetes-admin
kubectl config set-credentials kubernetes-admin \
  --client-certificate=/etc/kubernetes/pki/kubernetes-admin.pem \
  --client-key=/etc/kubernetes/pki/kubernetes-admin-key.pem \
  --embed-certs=true 
 
# 设置上下文参数，kubernetes-admin绑定kubernets集群
kubectl config set-context kubernetes-admin@kubernetes \
  --cluster=kubernetes \
  --user=kubernetes-admin
 
# 设置默认上下文
kubectl config use-context kubernetes-admin@kubernetes \
    --kubeconfig=kube.kubeconfig

# 自动生成在 ~/.kube/config
ll ~/.kube/
total 12
-rw------- 1 root root 10022 Mar 21 13:27 config

# 创建目录,存放admin kubeconfig
for node in node2 node3;do ssh $node "mkdir -p /root/.kube";done

# 分发复制
for node in node2 node3;do scp /root/.kube/config  $node:/root/.kube/config;done
```

#### 4.4. 查看集群组件状态
>上面步骤完成后，kubectl就可以与kube-apiserver通信了
```
kubectl cluster-info
kubectl get componentstatuses
kubectl get all --all-namespaces
```

#### 4.5. kubectl 命令补全
```
yum install -y bash-completion
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
source ~/.bashrc
```

### 5. 部署 kube-controller-manager
#### 5.1. kube-controller-manager 证书签名请求

> CN 对应 clusterrolebinding 的 system:kube-controller-manager 中subject 部分kind为user的用户名

```shell
cat > /root/k8s/ssl/kube-controller-manager-csr.json <<EOF
{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
    }
  ]
}
EOF
```
#### 5.2. 生成 kube-controller-manager 证书
```shell
cd /root/k8s/ssl/ && \
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client kube-controller-manager-csr.json | cfssljson --bare kube-controller-manager

# 分发到Master
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-controller-manager*.pem $node:/etc/kubernetes/pki/;done
```
#### 5.3. 创建kube-controller-manager kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=/etc/kubernetes/pki/kube-controller-manager.pem \
  --client-key=/etc/kubernetes/pki/kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-controller-manager@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-controller-manager@kubernetes \
  --kubeconfig=kube-controller-manager.kubeconfig


# 分发复制/etc/kubernetes/
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-controller-manager.kubeconfig  $node:/etc/kubernetes/;done
```

#### 5.4. kube-controller-manager配置文件

[启动参数](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-controller-manager/)

```shell
cat > /root/k8s/master/conf/kube-controller-manager.conf <<EOF
KUBE_CONTROLLER_MANAGER_ARGS="--bind-address=127.0.0.1 \
--allocate-node-cidrs=true \
--cluster-cidr=10.244.0.0/16 \
--cluster-name=kubernetes \
--root-ca-file=/etc/kubernetes/pki/ca.pem \
--client-ca-file=/etc/kubernetes/pki/ca.pem \
--cluster-signing-duration=43800h \
--cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
--cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
--authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
--authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
--kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
--leader-elect=true \
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
--use-service-account-credentials=true \
--service-account-private-key-file=/etc/kubernetes/pki/sa.key \
--service-cluster-ip-range=172.21.0.0/16 \
--controllers=*,bootstrapsigner,tokencleaner \
--alsologtostderr=true \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
```


#### 5.5. kube-controller-manager启动文件

```shell
cat > /root/k8s/master/systemd/kube-controller-manager.service <<EOF
[Unit]
Description=kube-controller-manager
Documentation=https://kubernetes.io/docs/
After=network-online.target
Wants=network-online.target

[Service]
EnvironmentFile=/etc/kubernetes/kube-controller-manager.conf
ExecStart=/usr/local/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_ARGS
LimitNOFILE=65536
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
```

#### 5.6.  启动 kube-controller-manager
```shell
# 分发启动文件到Master
for node in node1 node2 node3;do scp /root/k8s/master/conf/kube-controller-manager.conf $node:/etc/kubernetes/kube-controller-manager.conf;done
for node in node1 node2 node3;do scp /root/k8s/master/systemd/kube-controller-manager.service $node:/lib/systemd/system/kube-controller-manager.service;done

# 启动 kube-controller-manager
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-controller-manager --now";done

# 查看状态
systemctl status kube-controller-manager
journalctl -u kube-controller-manager
```

### 6. 部署 kube-scheduler
#### 6.1. kube-scheduler 证书签名请求

> CN 对应 clusterrolebinding 的 system:kube-scheduler 中subject部分kind为user的用户名

```shell
cat > /root/k8s/ssl/kube-scheduler-csr.json <<EOF
{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
    }
  ]
}
EOF
```

#### 6.2. 生成 kube-scheduler 证书
```shell
# 生成证书
cd /root/k8s/ssl/ && \
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client kube-scheduler-csr.json | cfssljson --bare kube-scheduler

# 分发到 Master
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-scheduler*.pem $node:/etc/kubernetes/pki/;done
```
#### 6.3. 创建 kube-scheduler 的kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-scheduler \
  --client-certificate=/etc/kubernetes/pki/kube-scheduler.pem \
  --client-key=/etc/kubernetes/pki/kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-scheduler@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-scheduler@kubernetes \
  --kubeconfig=kube-scheduler.kubeconfig


# 复制到 /etc/kubernetes
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-scheduler.kubeconfig $node:/etc/kubernetes/;done
```

#### 6.4 kube-scheduler 配置文件

[启动参数](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-scheduler/)

```shell
cat > /root/k8s/master/conf/kube-scheduler.conf <<EOF
KUBE_SCHEDULER_ARGS="--bind-address=127.0.0.1 \
--client-ca-file=/etc/kubernetes/pki/ca.pem \
--authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
--authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
--leader-elect=true \
--alsologtostderr=true \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
```

#### 6.5. kube-scheduler 启动文件

```shell
cat > /root/k8s/master/systemd/kube-scheduler.service <<EOF
[Unit]
Description=kube-scheduler
Documentation=https://kubernetes.io/docs/
After=network-online.target
Wants=network-online.target

[Service]
EnvironmentFile=/etc/kubernetes/kube-scheduler.conf
ExecStart=/usr/local/bin/kube-scheduler \$KUBE_SCHEDULER_ARGS
LimitNOFILE=65536
RestartSec=10
Restart=on-failure
[Install]
WantedBy=multi-user.target
EOF
```

#### 6.5.  启动 kube-scheduler
```shell
# 分发启动文件到Master
for node in node1 node2 node3;do scp /root/k8s/master/conf/kube-scheduler.conf $node:/etc/kubernetes/;done
for node in node1 node2 node3;do scp /root/k8s/master/systemd/kube-scheduler.service $node:/lib/systemd/system/kube-scheduler.service;done

# 启动 kube-scheduler
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-scheduler --now";done

# 查看状态
systemctl status kube-scheduler
journalctl -u kube-scheduler
```

### 7.TLS  Bootstraping 自动颁发证书

https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/

https://kubernetes.io/zh/docs/reference/access-authn-authz/bootstrap-tokens/

https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/

https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery.md

https://www.jianshu.com/p/b2f7340d40a7

```shell
# 允许kublet TLS bootstrap创建CSR请求
kubectl create clusterrolebinding create-csrs-for-bootstrapping \
  --clusterrole=system:node-bootstrapper \
  --group=system:bootstrappers

# 允许kube-controller-manager自动为system:bootstrappers组的用户颁发证书
kubectl create clusterrolebinding auto-approve-csrs-for-group \
  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \
  --group=system:bootstrappers

# 允许system:nodes组用户自动续期证书
kubectl create clusterrolebinding auto-approve-renewals-for-nodes \
  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \
  --group=system:nodes

# 创建bootstrap-token，在master节点上执行
cat > /root/k8s/ssl/create-bootstrap-token.sh <<EOF
#!/bin/bash
# 指定 apiserver 地址
KUBE_APISERVER="https://192.168.200.88:8443"

# 生成 Bootstrap Token
TOKEN_ID=$(cat /dev/urandom | tr -dc 0-9a-z | head -c 6)
TOKEN_SECRET=$(cat /dev/urandom | tr -dc 0-9a-z | head -c 16)
BOOTSTRAP_TOKEN="\${TOKEN_ID}.\${TOKEN_SECRET}"
echo "Bootstrap Token: \${BOOTSTRAP_TOKEN}"

# 使用secret存储token
mkdir -p /data/manifests/
cat > /data/manifests/bootstrap-secret.yaml <<HERE 
apiVersion: v1
kind: Secret
metadata:
  # 名字必须该格式
  name: bootstrap-token-\${TOKEN_ID}
  namespace: kube-system
# 必须该类型
type: bootstrap.kubernetes.io/token
stringData:
  description: "TLS引导令牌"
  token-id: \${TOKEN_ID}
  token-secret: \${TOKEN_SECRET}
  # expiration可选参数，设置token过期时间
  expiration: $(date --rfc-3339=seconds -d "6 hours" | sed 's/ /T/')
  usage-bootstrap-authentication: "true"
  usage-bootstrap-signing: "true"
  #auth-extra-groups: system:bootstrappers:default-node-token
HERE
kubectl apply -f /data/manifests/bootstrap-secret.yaml
EOF

bash /root/k8s/ssl/create-bootstrap-token.sh

```

### 8. 部署 kubelet

#### 8.1. 创建 kubelet-bootstrap.kubeconfig
```shell
cd /root/k8s/ssl/
# 生成 kubelet bootstrap-kubeconfig引导文件，注意：这里的${TOKEN_ID}和${BOOTSTRAP_TOKEN}改成上面TLS引导配置部分创建的值
TOKEN_ID=$(cat /data/manifests/bootstrap-secret.yaml | sed -n '/token-id/s#.*: ##p')
BOOTSTRAP_TOKEN=$(cat /data/manifests/bootstrap-secret.yaml | sed -n '/token-.*:/s#.*: ##p' | sed "N;s#\n#\.#")

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem \
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials "system:bootstrap:${TOKEN_ID}" \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user="system:bootstrap:${TOKEN_ID}" \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# kubelet-bootstrap.kubeconfig
for node in node1 node2 node3 node4 node5;do scp /root/k8s/ssl/kubelet-bootstrap.kubeconfig $node:/etc/kubernetes/;done
```

#### 8.2. kubelet 配置文件

[启动参数](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet/)

>"cgroupDriver": "cgroupfs",                     # 如果docker的驱动为systemd，处修改为systemd。此处设置很重要，否则后面node节点无法加入到集群 https://blog.csdn.net/Andriy_dangli/article/details/85062983
```shell
# 创建kubelet配置文件，即--config指定的配置，kubelet有些配置会逐渐的弃用命令行，改为这种方式加载
cat > /root/k8s/node/conf/kubelet.yaml <<EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  x509:
    clientCAFile: "/etc/kubernetes/pki/ca.pem"
  webhook:
    enabled: true
    cacheTTL: 0s
  anonymous:
    enabled: false
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
rotateCertificates: true
staticPodPath: /etc/kubernetes/manifests
clusterDomain: cluster.local.
clusterDNS:
- 172.21.0.2
maxPods: 110
EOF

# 创建kubelet配置文件
cat > /root/k8s/node/conf/kubelet.conf <<EOF
KUBELET_ARGS="--config=/etc/kubernetes/kubelet.yaml \
--bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
--network-plugin=cni \
--cni-conf-dir=/etc/cni/net.d \
--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 \
--alsologtostderr=true \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
```

#### 8.3. kubelet 启动文件
```shell
cat > /root/k8s/node/systemd/kubelet.service <<EOF
[Unit]
Description=Kubernetes kubelet
Documentation=https://kubernetes.io/docs/
After=network-online.target
Wants=network-online.target

[Service]
EnvironmentFile=/etc/kubernetes/kubelet.conf
ExecStartPre=/bin/mkdir -p /etc/kubernetes/manifests
ExecStart=/usr/local/bin/kubelet \$KUBELET_ARGS
LimitNOFILE=65536
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
```

#### 8.4. 分发配置文件和启动文件
```shell
# kubelet.yaml kubelet.conf
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/conf/kubelet* $node:/etc/kubernetes/;done

# kubelet.service
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kubelet.service $node:/lib/systemd/system//kubelet.service;done
```

#### 8.5. 启动 kubelet
```shell
# 启动，注意swapoff -a 需要关闭swap
for node in node1 node2 node3 node4 node5;do ssh $node "systemctl daemon-reload && systemctl enable kubelet --now";done

# 查看状态
systemctl status kubelet
journalctl -u kubelet
```

#### 8.6. workder加入集群
>kubelet服务启动成功后，api会自动Approve一下bootstrap请求。worker节点自动加入集群
```shell
kubectl get csr

NAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-9XZBBrGZ3ZcZjC9lqIdRl2Fn0gvhfKuAMeezRgXql8A   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-AXEZ65YFmt_vKVq8puhpZJgTMOi12Uzq0WUz7oTQ6ZE   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-J8uS6hmZYETrDwHeRHeBNtauyCOQx82WuxUbXkp7Sh0   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-XaPhQT-q14lOzgsRTm1Z6MWiKXRhnL-ByadXlithFs0   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-sK7uG0Jw7XRR4If2o3bdTswEofy9f0tAANSIxmQ2zLM   5m35s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending

# 查看节点，此时的状态是NotReady的，因为我们的kube-proxy和网络组件没有安装
kubectl get nodes
NAME           STATUS     ROLES    AGE     VERSION
docker-k8s01   NotReady   <none>   2m31s   v1.20.1
docker-k8s02   NotReady   <none>   2m31s   v1.20.1
docker-k8s03   NotReady   <none>   2m30s   v1.20.1
docker-k8s04   NotReady   <none>   2m30s   v1.20.1
docker-k8s05   NotReady   <none>   2m31s   v1.20.1

```

### 9.  部署 kube-proxy
#### 9.1. kube-proxy 证书签名请求

> CN 对应 clusterrolebinding 的 system:node-proxier中subject部分kind为user的用户名

```shell
cat >/root/k8s/ssl/kube-proxy-csr.json  <<'HERE'
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
    }
  ]
}
HERE
```
#### 9.2. 生成 kube-proxy 证书
```shell
# 生成证书
cd /root/k8s/ssl/ && \
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client kube-proxy-csr.json | cfssljson --bare kube-proxy

# 分发到 node节点
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kube-proxy*.pem $node:/etc/kubernetes/pki/;done
```
#### 9.3. 创建 kube-proxy 的kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/pki/ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-proxy \
  --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \
  --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-proxy@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-proxy@kubernetes \
  --kubeconfig=kube-proxy.kubeconfig
  
# 分发
for node in node1 node2 node3 node4 node5;do scp /root/k8s/ssl/kube-proxy.kubeconfig $node:/etc/kubernetes/;done
```

#### 9.4. kube-proxy 配置文件

[启动参数](https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-proxy/)

>clusterCIDR: 10.244.0.0/16                           # 此处网段必须与pod网段保持一致，否则部署
```shell
cat > /root/k8s/node/conf/kube-proxy.yaml <<'HERE'
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: "0.0.0.0"
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: "10.244.0.0/16"
mode: "ipvs"
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: "rr"
  strictARP: false
  syncPeriod: 30s
  tcpFinTimeout: 0s
  tcpTimeout: 0s
  udpTimeout: 0s
HERE
```

#### 9.5. kube-proxy 启动文件
```shell
cat > /root/k8s/node/systemd/kube-proxy.service <<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

#### 9.6. 分发配置文件和启动文件
```shell
# kube-proxy.yaml
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/conf/kube-proxy.yaml $node:/etc/kubernetes/kube-proxy.yaml;done

# kube-scheduler.service
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kube-proxy.service $node:/lib/systemd/system/;done
```

#### 9.7. 启动 kube-proxy
```shell
# 启动
for node in node1 node2 node3 node4 node5;do ssh $node "systemctl daemon-reload && systemctl enable kube-proxy --now";done

# 查看状态
systemctl status kube-proxy
journalctl -u kube-proxy
```

## 部署网络组件
>此时的状态节点都是NoReady，是因为网络组件没有配置，配置起来Ready了

### 1. Calico

以下安装根据实际情况选择一种即可

#### 1.1 Api 数据存储50节点
> 使用kubernetes Api数据存储库（不超过50个节点）安装calico
> 如果pod网络不是使用192.168.0.0/16则取消CALICO_IPV4POOL_CIDR变量的注释，并把值设置为pod的网段
> 此种方式卸载的calico-kube-controllers会一直Terminating的状态
> https://stackoverflow.com/questions/61672804/after-uninstalling-calico-new-pods-are-stuck-in-container-creating-state

```shell
mkdir -p /data/manifests/ && cd /data/manifests/
curl https://docs.projectcalico.org/manifests/calico.yaml -o calico.yaml

sed -i '/CALICO_IPV4POOL_CIDR/,+1c\            - name: CALICO_IPV4POOL_CIDR\n              value: "10.244.0.0/16"' calico.yaml

# 部署calico
kubectl apply -f calico.yaml

# 此时的节点都是 Ready
kubectl get nodes
```

#### 1.2 Api 数据存储50以上节点

```shell
# 使用kubernetes Api数据存储库（超过50个节点）安装calico
curl https://docs.projectcalico.org/manifests/calico-typha.yaml -o calico-typha.yaml

# 如果pid网络不是使用192.168.0.0/16则取消CALICO_IPV4POOL_CIDR变量的注释，并把值设置为pod的网段
sed -i '/CALICO_IPV4POOL_CIDR/,+1c\            - name: CALICO_IPV4POOL_CIDR\n              value: "10.244.0.0/16"' calico-typha.yaml

# 部署calico
kubectl apply -f calico-typha.yaml
```

#### 1.3 etcd 数据存储

```shell
# 使用etcd存储安装calico
curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico.yaml

# 如果pid网络不是使用192.168.0.0/16则取消CALICO_IPV4POOL_CIDR变量的注释，并把值设置为pod的网段
sed -i '/CALICO_IPV4POOL_CIDR/,+1c\            - name: CALICO_IPV4POOL_CIDR\n              value: "10.16.0.0/12"' calico.yaml

# 修改etcd TLS认证配置
sed -i "/# etcd-key/c\  etcd-key: $(cat /etc/kubernetes/pki/kube-apiserver-etcd-client-key.pem | base64 -w 0)/" calico-etcd.yaml 
sed -i "/# etcd-cert/c\  etcd-cert: $(cat /etc/kubernetes/pki/kube-apiserver-etcd-client.pem | base64 -w 0)/" calico-etcd.yaml 
sed -i "/# etcd-ca/c\  etcd-key: $(cat /etc/etcd/pki/etcd-ca.pem | base64 -w 0)/" calico-etcd.yaml

sed -i \
'/  etcd_endpoints:/c\  etcd_endpoints: "https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379"' \
calico-etcd.yaml

sed -i '/etcd_ca:/c\  etcd_ca: "/calico-secrets/etcd-ca"' calico-etcd.yaml
sed -i '/etcd_cert:/c\  etcd_cert: "/calico-secrets/etcd-cert"' calico-etcd.yaml
sed -i '/etcd_key:/c\  etcd_key: "/calico-secrets/etcd-key"' calico-etcd.yaml

# 部署calico
kubectl apply -f calico.yaml
```

### 2. Flannel
```shell
cd /data/manifests

# 创建网络需要的目录
mkdir /opt/cni/bin/ && mkdir /etc/cni/net.d

# 下载cni组件
wget https://github.com/containernetworking/plugins/releases/download/v0.8.7/cni-plugins-linux-amd64-v0.8.7.tgz
tar -zxvf cni-plugins-linux-amd64-v0.8.7.tgz -C /opt/cni/bin/

# 下载flannel yaml
curl https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml -o flannel.yml

# 如果pod网段不一样需要修改
sed -i 's@10.244.0.0/16@10.244.0.0/16@' flannel.yml

# 部署flannel
kubectl apply -f flannel.yml
kubectl get pods -A
```

## 部署 Coredns
>需要修改的地方：
>
>kubernetes cluster.local in-addr.arpa ip6.arpa
>forward . /etc/resolv.conf
>clusterIP为：172.21.0.2（kubelet配置文件中的clusterDNS）
```shell
cd /data/manifests

curl https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed -o coredns.yaml

sed -i -e 's@kubernetes CLUSTER_DOMAIN REVERSE_CIDRS@kubernetes cluster.local in-addr.arpa ip6.arpa@' -e 's@forward . UPSTREAMNAMESERVER@forward . /etc/resolv.conf@' -e 's@STUBDOMAINS@@' -e 's@clusterIP: CLUSTER_DNS_IP@clusterIP: 172.21.0.2@' coredns.yaml

kubectl apply -f coredns.yaml
```

https://www.cnblogs.com/technology178/p/14295776.html
https://www.cnblogs.com/dinghc/p/13031436.html

## 部署 Metrics-server

```shell
cd /data/manifests

# 安装metrics-server
curl -L https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.2/components.yaml -o metrics-server.yaml

# 跳过kubelet服务端TLS认证，在kubelet没有启用服务端验证的情况会不修改metrics-server会启动不起来，可以不修改的情况下查看日志可以发现
sed -i '/secure-port/a\        - --kubelet-insecure-tls' metrics-server.yaml

kubectl apply -f metrics-server.yaml

# 查看运行状态
kubectl get pods -l k8s-app=metrics-server -n kube-system

# 验证是否正常收集集群度量值
kubectl top nodes
kubectl top pods -A
```

## 部署 Dashboard

```shell
cd /data/manifests

# 安装Dashboad
curl https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml -o kubernetes-dashboard.yaml
kubectl apply -f kubernetes-dashboard.yaml

# 可以将service改成nodeport不然没法访问，否则需要部署ingress
kubectl edit svc/kubernetes-dashboard -n kubernetes-dashboard

# Dashboad登录认证
# 创建ServiceAccout dashboard-admin授予ServicAccout dashboard-admin管理员权限，注意：生产环境谨慎使用管理员权限，建议授权特定某个名称空间权限
cat > /root/manifests/dashboard-admin.yaml << EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: kubernetes-dashboard
  name: dashboard-admin
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - name: dashboard-admin
    kind: ServiceAccount
    namespace: kubernetes-dashboard
EOF

kubectl apply -f dashboard-admin.yaml


# Token方式，在Dashboad UI将选择Token登录方式，然后将打印出的token在Dashboad UI中填入登录
SECRET=$(kubectl get sa -n kubernetes-dashboard dashboard-admin -o jsonpath={.secrets[0].name})
kubectl get secret ${SECRET} -n kubernetes-dashboard -o jsonpath={.data.token} | base64 -d	
# 将打印出的token在Dashboad UI使用登录

# kubeconfig方式，在Dashboad UI将选择Kubeconfig登录方式，然后选择下面生成的kubeconfig
SECRET=$(kubectl get sa -n kubernetes-dashboard dashboard-admin -o jsonpath={.secrets[0].name})
TOKEN=$(kubectl get secret ${SECRET} -n kubernetes-dashboard -o jsonpath={.data.token} | base64 -d)

kubectl config set-cluster kubernetes \
--server=https://192.168.200.88:8443 \
--embed-certs=true \
--certificate-authority=/etc/kubernetes/pki/ca.pem \
--kubeconfig=/etc/kubernetes/dashboard-admin.kubeconfig

kubectl config set-credentials dashboard-admin --token=${TOKEN} --kubeconfig=/etc/kubernetes/dashboard-admin.kubeconfig

kubectl config set-context dashboard-admin \
--cluster=kubernetes \
--user=dashboard-admin \
--kubeconfig=/etc/kubernetes/dashboard-admin.kubeconfig

kubectl config use-context dashboard-admin --kubeconfig=/etc/kubernetes/dashboard-admin.kubeconfig
```

## 部署 kuboard
[REFERENCE](https://kuboard.cn/install/install-dashboard.html)
```shell
cd /data/manifests/

curl https://kuboard.cn/install-script/kuboard.yaml -o kuboard.yaml

kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml

# 获取Token，此Token拥有ClusterAdmin的权限
echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk '{print $1}') -o go-template='{{.data.token}}' | base64 -d)



```

## 集群验证

> 必须要做，有可能网络问题之类的,DNS

### 1. 安装 busybox
```shell
cd /root/manifests

cat > /root/manifests/busybox.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
EOF
```
### 2. 验证解析
> Pod 必须能解析 Service
> Pod 必须能解析跨 namespace 的Service
> 每个节点都必须要能访问 Kubernetes 的 kubernets svc 443 和kube-dns的Service 53
> 每个Pod和Pod之间要能通讯 1. 同namespace 2.跨namespace 3.跨机器通信

```shell
# Pod 必须能解析 Service，解析跨 namespace 的Service
kubectl exec busybox -n default -- nslookup kubernetes
kubectl exec busybox -n default -- nslookup kube-dns.kube-system

# 每个节点都必须要能访问 Kubernetes 的 kubernets svc 443 和kube-dns的Service 53
kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   172.21.0.1   <none>        443/TCP   4h10m

telnet 172.21.0.1 443

kubectl get svc -n kube-system
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
kube-dns         ClusterIP   172.21.0.2       <none>        53/UDP,53/TCP,9153/TCP   12m

telnet 172.21.0.2 53

# 每个Pod和Pod之间要能通讯 1. 同namespace 2.跨namespace 3.跨机器通信
```

### 3. 验证deploy

```shell
kubectl create deploy nginx --image=nginx --replicas=3
kubectl get pods -o wide
kubectl delete deploy nginx
kubectl delete pod busybox
```

验证完，就可以使用了。


















