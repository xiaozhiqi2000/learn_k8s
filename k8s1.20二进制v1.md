[TOC]

# K8S 集群部署

[REFERENCE]<https://kubernetes.io/docs/home/>

## 环境介绍

节点部署
| 主机名 | IP | 配置 | Role | 备注 |
| ---- | ------ | ---- | ---- | ------- |
| docker-k8s01 | 192.168.200.8 | 2C4G | Master,Etcd | kube-apiserver、kube-controller-manager、kube-scheduler |
| docker-k8s02 | 192.168.200.9 | 2C4G | Master,Etcd | kube-apiserver、kube-controller-manager、kube-scheduler |
| docker-k8s03 | 192.168.200.10 | 2C4G | Master,Etcd | kubelet、kube-proxy |
| docker-k8s04 | 192.168.200.11 | 2C4G | Node,Etcd | kubelet、kube-proxy |
| docker-k8s05 | 192.168.200.12 | 2C4G | Node,Etcd | kubelet、kube-proxy |
| VIP | 192.168.200.88 |   |   | kube-apiserver VIP |

网络部署
|网络 | IP段 |
| ---- | ------ |
| 物理网络 | 192.168.200.0/24 |
| POD网络 | 10.244.0.0/16 |
| SERVICE网络 | 172.21.0.0/16 |


依赖组件
|  组件名称  |  组件版本  |
| ------------- | ----------- |
| Etcd |  |
| Docker | |
| Go | |
| CNI | |
| CSI | |
| Dashboard |  |
| Heapster | |
| Cluster | |
| kube-dns | |
| Influxdb | |
| Grafana | |
| Kibana | |
| cAdvisor | |
| Fluentd | |
| Elasticsearch | |
| go-oidc | |
| calico | |
| crictl | |
| CoreDNS | |
| event-exporter | |
| metrics-server | |
| ingress-gce | |
| ingress-nginx | |
| ip-masq-agent | |
| hcsshim | |

## 环境初始化（所有节点)
```shell
# 配置Yum源
rm -f /etc/yum.repos.d/* && wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo && wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo && yum clean all && yum makecache fast

# 升级软件包,Centos7需要升级，Centos8可以按需升级系统
yum update -y --exclude=kernel* && reboot

# 密钥互通node1分发
ssh-keygen -P "" -f /root/.ssh/id_rsa
cat /root/.ssh/id_rsa.pub > /root/.ssh/authorized_keys
for i in {2..5};do scp -r .ssh node$i:~/ ;done

# 主机名解析
cat > /etc/hosts <<EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.200.8 docker-k8s01 node1
192.168.200.9 docker-k8s02 node2
192.168.200.10 docker-k8s03 node3
192.168.200.11 docker-k8s04 node4
192.168.200.12 docker-k8s05 node5
EOF

# 关闭防火墙
systemctl disable firewalld;systemctl stop firewalld

# 禁用SELinux，允许容器访问宿主机的文件系统
setenforce 0
sed -i 's@SELINUX=.*@SELINUX=disabled@' /etc/selinux/config

# 关闭 you have new mail in
echo "unset MAILCHECK">> /etc/profile;source /etc/profile

# 同步时间
yum -y install chrony ntpdate
systemctl start chronyd && systemctl enable chronyd
ntpdate ntp1.aliyun.com

# 关闭SWAP
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab
echo "vm.swappiness = 0">> /etc/sysctl.conf  && sysctl -p
```

## 升级内核（所有节点)
```shell
# 升级内核,安装长期支持版本，建议升级 4.18+ 不要太新
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm
yum --enablerepo=elrepo-kernel install kernel-lt -y

grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg

# 开启 User namespaces
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
grep user_namespace /boot/grub2/grub.cfg

# 查看版本
grubby --default-kernel

# 重启
reboot

# 查看内核版本
uname -a
Linux docker-k8s01 4.4.118-1.el7.elrepo.x86_64 #1 SMP Sun Feb 25 19:10:14 EST 2018 x86_64 x86_64 x86_64 GNU/Linux

# 安装 ipvs 软件包
yum install -y ipvsadm ipset sysstat contrack libseccomp net-tools

# 加载ipvs模块,在内核 4.19+版本nf_contrack_ipv4已经改为nf_contrack，4.18以下使用 nf_contrack_ipv4即可
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack

# 加入以下内容,在内核 4.19+版本nf_contrack_ipv4已经改为nf_contrack，4.18以下使用 nf_contrack_ipv4即可
cat > /etc/modules-load.d/ipvs.conf <<EOF
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
# 开启这个服务
systemctl enable --now systemd-modules-load.service

# 加载br_netfilter iptables 模块
cat <<EOF >  /etc/sysctl.d/k8s.conf
br_netfilter
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory = 1
vm.panic_on_oom = 0
fs.inotify.max_user_watchs = 89100
fs.file-max = 52706963
fs.nr_open = 52706963
net.netfilter.nf_conntrack_max = 2310720
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_sysncookies = 1
net.ipv4.tcp_max_sys_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_sys_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF

# 重新加载
sysctl --system

# 查看是否加载上ipvs模块
lsmod | grep -e ip_vs -e nf_conntrack

```

## 安装 Docker（所有节点)
[Docker官网安装文档](https://docs.docker.com/engine/install/)
[Docker镜像国内](https://kubernetes.feisky.xyz/appendix/mirrors)

```shell
# 下载阿里的docker源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo

# 安装 docker 19.03.15
yum -y install docker-ce-19.03.15-3.el7 docker-ce-cli-19.03.15-3.el7 containerd.io-1.4.3-3.1.el7 

# 配置docker目录
mkdir -p /data/docker /etc/docker /etc/systemd/system/docker.service.d

# 配置国内镜像加速
cat > /etc/docker/daemon.json << EOF
{
  "graph": "/data/docker",
  "storage-driver": "overlay2",
  "registry-mirrors": [
      "https://0mmxqss3.mirror.aliyuncs.com",
      "https://docker.mirrors.ustc.edu.cn",
      "https://registry.docker-cn.com"
  ],
  "insecure-registries": ["registry.cn-hangzhou.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "live-restore": true,
  "log-driver": "json-file",
  "log-level": "warn",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
EOF

# 启动dcoker
systemctl start docker && systemctl enable docker --now && systemctl status docker
```

## 安装HA(master节点)

> haproxy + keepalived 也可以 nginx + keepalived 四层代理

### 1. haproxy

```shell
# 安装 haproxy 依赖
yum install -y haproxy keepalived libnl3-devel ipset-devel

# 配置 haproxy，注意修改成自己的IP
cat > /etc/haproxy/haproxy.cfg <<EOF
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /var/run/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master
        bind 0.0.0.0:8443
        mode tcp
        option tcplog
        balance source
        server s1 192.168.200.8:6443  check inter 10000 fall 2 rise 2 weight 1
        server s2 192.168.200.9:6443  check inter 10000 fall 2 rise 2 weight 1
        server s3 192.168.200.10:6443  check inter 10000 fall 2 rise 2 weight 1
EOF

# 分发其他节点
for node in node2 node3;do scp /etc/haproxy/haproxy.cfg $node:/etc/haproxy/;done

# 启动 haproxy
for node in node1 node2 node3;do ssh ${node} "systemctl start haproxy && systemctl enable haproxy --now";done

# 查看下服务情况
systemctl status haproxy
```

### 2. keepalived
```shell
# 配置 keepalived，注意修改成自己的IP和网卡名称
# master1 配置

cat > /etc/keepalived/keepalived.conf <<EOF
global_defs {
    script_user root 
    enable_script_security
    router_id lb-master
}

vrrp_script check-haproxy {
    script "/bin/bash -c 'if [[ $(netstat -nlp | grep 8443) ]]; then exit 0; else exit 1; fi'"
    interval 2
    weight -50
}

vrrp_instance VI-kube-master {
    state MASTER
    priority 120
    dont_track_primary
    interface ens33
    virtual_router_id 120
    advert_int 3
    track_script {
        check-haproxy
    }
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.200.88
    }
}

# notify "/container/service/keepalived/assets/notify.sh"
EOF

# 分发 keepalived
for node in node2 node3;do scp /etc/keepalived/keepalived.conf ${node}:/etc/keepalived/;done

# master2 修改
sed -i -e 's@state MASTER@state BACKUP@' -e 's@priority 120@priority 90@' /etc/keepalived/keepalived.conf

# master3 修改
sed -i -e 's@state MASTER@state BACKUP@' -e 's@priority 120@priority 100@' /etc/keepalived/keepalived.conf


# 启动 keepalived
for node in node1 node2 node3;do ssh ${node} "systemctl start keepalived && systemctl enable keepalived --now";done


# 查看下服务情况
systemctl status keepalived
```

## CA证书准备

### 1.  下载 CFSSL 证书工具

```shell
wget -O /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 
wget -O /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget -O /usr/local/bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

chmod +x  /usr/local/bin/cfssl*
```

### 2.  准备相关目录

```shell
mkdir /root/k8s/ssl/ -p
```

### 3. CA 配置文件

```shell
cat > /root/k8s/ssl/ca-config.json  <<'HERE'
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "87600h"
      }
    }
  }
}
HERE
```

### 4. kubernetes ca 证书请求

```shell
cat > /root/k8s/ssl/kubernetes-ca-csr.json  <<'HERE'
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
HERE
```

### 5.pront-proxy ca 证书请求

```shell
cat > /root/k8s/ssl/front-proxy-ca-csr.json <<EOF
{
    "CN": "front-proxy-ca",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen"
        }
    ],
    "ca": {
        "expiry": "876000h"
    }
}
EOF
```

### 6. 生成证书

```shell
# 生成 kubernetes CA证书和密钥
cd /root/k8s/ssl/ && \
cfssl gencert --initca=true kubernetes-ca-csr.json | cfssljson --bare ca

# 生成 front-proxy CA证书和密钥
cfssl gencert --initca=true front-proxy-ca-csr.json | cfssljson --bare front-proxy-ca

# 分发到 Master 节点
for node in node1 node2 node3;do scp -r /root/k8s/ssl/ca*.pem /root/k8s/ssl/front-proxy-ca*.pem $node:/etc/kubernetes/pki/;done && cd
```

## Etcd 集群部署（node01）

### 1. 相关目录准备

```shell
mdir -p /root/k8s/etcd/{bin,systemd}

# 创建证书目录和数据目录
for node in node1 node2 node3 node4 node5;do ssh ${node} "mkdir -p /etc/etcd/ssl/ /var/lib/etcd";done
```

### 2. etcd 证书签名请求
```shell
cat > /root/k8s/ssl/etcd-csr.json  <<'HERE'
{
  "CN": "etcd",
  "hosts": [
    "127.0.0.1",
    "192.168.200.8",
    "192.168.200.9",
    "192.168.200.10"
  ],
  "key": {
    "algo": "rsa",
     "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
HERE
```

### 3. 生成etcd证书

```shell
cd /root/k8s/ssl/
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes etcd-csr.json | cfssljson --bare etcd
```

### 4. 下载etcd并解压
```shell
cd ~
wget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz
tar xf etcd-v3.4.13-linux-amd64.tar.gz
cp -f /root/etcd-v3.4.13-linux-amd64/etcd* /root/k8s/etcd/bin/
```

### 5. 创建etcd启动文件
```shell
cat > /root/k8s/etcd/systemd/etcd.service <<'HERE'
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
--name=etcd01 \
--trusted-ca-file=/etc/etcd/ssl/ca.pem \
--cert-file=/etc/etcd/ssl/etcd.pem \
--key-file=/etc/etcd/ssl/etcd-key.pem \
--peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \
--peer-cert-file=/etc/etcd/ssl/etcd.pem \
--peer-key-file=/etc/etcd/ssl/etcd-key.pem \
--initial-advertise-peer-urls=https://192.168.200.8:2380 \
--listen-peer-urls=https://192.168.200.8:2380 \
--listen-client-urls=https://192.168.200.8:2379,http://127.0.0.1:2379 \
--advertise-client-urls=https://192.168.200.8:2379 \
--initial-cluster-token=etcd-cluster \
--initial-cluster=etcd01=https://192.168.200.8:2380,etcd02=https://192.168.200.9:2380,etcd03=https://192.168.200.10:2380 \
--initial-cluster-state=new \
--data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
HERE
```

### 6. 下发etcd二进制文件
```shell
#分发证书密钥
for node in node1 node2 node3;do scp -f /root/k8s/ssl/ca*.pem /root/k8s/ssl/etcd*.pem $node:/etc/etcd/ssl/;done

# 分发etcd命令
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/bin/ ${node}:/usr/local/bin/;done

# 下发etcd所需的service文件
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/systemd/ ${node}:/lib/systemd/system/;done

#修改service相应节点IP(node2,node3节点执行)
# node2 操作
sed -i -e 's@192.168.200.8@192.168.200.9@g' -e 's@etcd01@etcd02@' -e 's@--initial-cluster=etcd02=https://192.168.200.9:2380@--initial-cluster=etcd01=https://192.168.200.8:2380@g' /lib/systemd/system/etcd.service

# node3 操作
sed -i -e 's@192.168.200.8@192.168.200.10@g' -e 's@etcd01@etcd03@' -e 's@--initial-cluster=etcd03=https://192.168.200.10:2380@--initial-cluster=etcd01=https://192.168.200.8:2380@g' /lib/systemd/system/etcd.service
```

### 7. 启动etcd集群
```shell
# 启动etcd集群,由于集群需要保证quorum机制，因此至少有两个节点启动服务，集群初始化启动成功
systemctl daemon-reload && systemctl enable etcd --now
```

### 8. 检查集群健康
```shell
# node1,node2,node3,随便1个节点执行
ETCDCTL_API=3 etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem\
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 endpoint health
 
ETCDCTL_API=3 etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem\
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 member list
  
ETCDCTL_API=3 etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem  \
  --cert=/etc/etcd/ssl/etcd.pem   \
  --key=/etc/etcd/ssl/etcd-key.pem   \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 endpoint status

```

## Kubernetes 组件部署

### 1. 准备相关目录
```shell
# 创建 master worker 命令启动文件目录
mkdir -p /root/k8s/{master,node}/{bin,systemd}

for node in node1 node2 node3 node4 node5;do ssh ${node} "mkdir -p /etc/kubernetes/manifests/ /var/lib/kubelet /var/log/kubernetes";done

# 创建 master worker 证书目录
for node in node1 node2 node3 node4 node5;do ssh ${node} "mkdir -p /etc/kubernetes/pki/";done
```

### 2. 下载 kubernetes 二进制
```shell

# 下载k8s 1.20.1二进制包
wget https://dl.k8s.io/v1.20.1/kubernetes-server-linux-amd64.tar.gz

# 解压
tar xf kubernetes-server-linux-amd64.tar.gz && tar xf kubernetes/kubernetes-src.tar.gz -C kubernetes/

# 根据master，node进行归类
cp -f /root/kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /root/k8s/master/bin/

cp -f /root/kubernetes/server/bin/{kubelet,kube-proxy} /root/k8s/node/bin/

# 下发master二进制文件
for node in node1 node2 node3;do rsync -avP /root/k8s/master/bin/ ${node}:/usr/local/bin/;done

# 下发worker二进制文件
for node in node1 node2 node3 node4 node5;do rsync -avP /root/k8s/node/bin/ ${node}:/usr/local/bin/;done
```

### 3. 部署kube-apiserver 
>注意：此处需要将dns首ip、etcd、master、node、service、VIP的ip都填上

#### 3.1. kube-apiserver 签名请求
```shell
cat > /root/k8s/ssl/kube-apiserver-csr.json  <<'HERE'
{
  "CN": "kube-apiserver",
  "hosts": [
    "127.0.0.1",
    "localhost",
    "192.168.200.8",
    "192.168.200.9",
    "192.168.200.10",
    "192.168.200.11",
    "192.168.200.12",
    "192.168.200.88",
    "172.21.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
HERE
```

#### 3.2. front-proxy-client 签名请求

> 用于api聚合

```shell
cat > /root/k8s/ssl/front-proxy-client-csr.json <<EOF
{
  "CN": "front-proxy-client",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen"
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
```

#### 3.3. 生成证书

```shell
cd /root/k8s/ssl/

# 生成 kube-apiserver、front-proxy-client证书和密钥
cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes kube-apiserver-csr.json | cfssljson --bare kube-apiserver

cfssl gencert -ca front-proxy-ca.pem -ca-key front-proxy-ca-key.pem -config ca-config.json -profile kubernetes front-proxy-client-csr.json | cfssljson -bare front-proxy-client

# 分发证书
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-apiserver*.pem /root/k8s/ssl/front-proxy-client*.pem $node:/etc/kubernetes/pki/;done
```

#### 3.4. 生成token文件

```shell
cat > /root/k8s/ssl/token.csv <<EOF
$(head -c 16 /dev/urandom | od -An -t x | tr -d ' '),kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF

for node in node1 node2 node3;do scp -r /root/k8s/ssl/token.csv $node:/etc/kubernetes/pki/;done
```

#### 3.5 生成serviceaccout使用的私钥和公钥
```shell
openssl genrsa -out /root/k8s/ssl/sa.key 2048
openssl rsa -in /root/k8s/ssl/sa.key -pubout -out /root/k8s/ssl/sa.pub

for node in node1 node2 node3;do scp -r /root/k8s/ssl/sa* $node:/etc/kubernetes/pki/;done
```


#### 3.6. kube-apiserver 启动文件

```shell
cat > /root/k8s/master/systemd/kube-apiserver.service  <<'HERE'
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --bind-address=0.0.0.0 \
  --secure-port=6443 \
  --insecure-port=0 \
  --advertise-address=192.168.200.8 \
  --service-cluster-ip-range=172.21.0.0/16 \
  --service-node-port-range=30000-32767 \
  --authorization-mode=RBAC,Node \
  --enable-bootstrap-token-auth=true \
  --token-auth-file=/etc/kubernetes/pki/token.csv \
  --client-ca-file=/etc/kubernetes/pki/ca.pem \
  --service-account-key-file=/etc/kubernetes/pki/sa.pub \
  --service-account-signing-key-file=/etc/kubernetes/pki/sa.key  \
  --service-account-issuer=https://kubernetes.default.svc.cluster.local \
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota \
  --tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem \
  --tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem \
  --kubelet-client-certificate=/etc/kubernetes/pki/kube-apiserver.pem \
  --kubelet-client-key=/etc/kubernetes/pki/kube-apiserver-key.pem \
  --kubelet-preferred-address-types=InternalIP,ExternnalIP,Hostname \
  --etcd-cafile=/etc/etcd/ssl/ca.pem \
  --etcd-certfile=/etc/etcd/ssl/etcd.pem \
  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \
  --etcd-servers=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 \
  --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \
  --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
  --requestheader-allowed-names=aggregator \
  --requestheader-group-headers=X-Remote-Group \
  --requestheader-extra-headers-prefix=X-Remote-Extra- \
  --requestheader-username-headers=X-Remote-User \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --event-ttl=1h \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=4
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
HERE
```

#### 3.7. 分发 kube-apiserver 启动文件
```shell
#分发ca、apiserver证书密钥、启动文件
for node in node1 node2 node3;do scp -f /root/k8s/ssl/ca*.pem /root/k8s/ssl/etcd*.pem $node:/etc/kubernetes/pki/;done

for node in node1 node2 node3;do scp -r /root/k8s/master/systemd/kube-apiserver.service $node:/lib/systemd/system/;done

# node2修改IP
sed -i -e 's@192.168.200.8@192.168.200.9@g' -e 's@192.168.200.9:2379@192.168.200.8:2379@' /lib/systemd/system/kube-apiserver.service

# node3修改IP
sed -i -e 's@192.168.200.8@192.168.200.10@g' -e 's@192.168.200.10:2379@192.168.200.8:2379' /lib/systemd/system/kube-apiserver.service
```

#### 3.8. 启动 kube-apiserver
```shell
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-apiserver --now && systemctl status kube-apiserver";done

# 有返回则正常
curl --insecure https://192.168.200.8:6443/

```

### 4. 部署 kubectl

#### 4.1. kubernetes-admin-csr.json

```shell
cat > /root/k8s/ssl/admin-csr.json <<EOF
{
    "CN": "admin",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen",
            "O": "system:masters",
            "OU": "System"
        }
    ]
}
EOF
```

#### 4.2. 生成 kubectl 证书
```shell
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes admin-csr.json | cfssljson --bare admin

# 分发证书
for node in node1 node2 node3;do scp -r /root/k8s/ssl/admin*.pem $node:/etc/kubernetes/pki/;done
```

#### 4.3. 生成admin kubeconfig
>生成集群管理员admin kubeconfig配置文件供kubectl调用，kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书
```shell
# 设置一个集群叫 kubernetes
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube.kubeconfig
 
# 设置一个用户叫admin
kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kube.kubeconfig
 
# 设置上下文参数，admin绑定kubernets集群
kubectl config set-context admin@kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kube.kubeconfig
 
# 设置默认上下文
kubectl config use-context admin@kubernetes \
    --kubeconfig=kube.kubeconfig

# 创建目录,存放admin kubeconfig
for node in node1 node2 node3;do ssh $node "mkdir -p /root/.kube";done

# 分发复制
for node in node1 node2 node3;do scp /root/k8s/ssl/kube.kubeconfig  $node:/root/.kube/config;done
```

#### 4.4.授权访问kubelet api权限

> 授权kubernetes证书访问kubelet api权限

```shell
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
```

#### 4.5. 查看集群组件状态
>上面步骤完成后，kubectl就可以与kube-apiserver通信了
```
kubectl cluster-info
kubectl get componentstatuses                                        
kubectl get all --all-namespaces
```

#### 4.6. kubectl 命令补全
```
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
kubectl completion bash > ~/.kube/completion.bash.inc
source '/root/.kube/completion.bash.inc'
echo "source '/root/.kube/completion.bash.inc'" >> $HOME/.bash_profile
```

### 5. 部署 kube-controller-manager
#### 5.1. kube-controller-manager 签名请求
```shell
cat > /root/k8s/ssl/kube-controller-manager-csr.json  <<'HERE'
{
  "CN": "system:kube-controller-manager",
  "hosts": [
    "127.0.0.1",
    "192.168.200.8",
    "192.168.200.9",
    "192.168.200.10"
  ],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen",
      "O": "system:kube-controller-manager",
      "OU": "System"
    }
  ]
}
HERE
```
#### 5.2. 生成 kube-controller-manager 证书
```shell
# 生成证书和密钥
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes kube-controller-manager-csr.json | cfssljson --bare kube-controller-manager

# 分发到master
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-controller-manager*.pem $node:/etc/kubernetes/pki/;done
```
#### 5.3. 创建kube-controller-manager kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-controller-manager@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-controller-manager@kubernetes \
  --kubeconfig=kube-controller-manager.kubeconfig

# 分发复制/etc/kubernetes/
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-controller-manager.kubeconfig  $node:/etc/kubernetes/;done
```

#### 5.4. kube-controller-manager启动文件
```shell
cat > /root/k8s/master/systemd/kube-controller-manager.service <<'HERE'
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --bind-address=127.0.0.1 \
  --port=0
  --service-cluster-ip-range=172.21.0.0/16 \
  --cluster-cidr=10.244.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
  --client-ca-file=/etc/kubernetes/pki/ca.pem \
  --root-ca-file=/etc/kubernetes/pki/ca.pem \
  --tls-cert-file=/etc/kubernetes/pki/kube-controller-manager.pem \
  --tls-private-key-file=/etc/kubernetes/pki/kube-controller-manager-key.pem \
  --service-account-private-key-file=/etc/kubernetes/pki/sa.key \
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
  --use-service-account-credentials=true \
  --node-monitor-grace-period=40s \
  --node-monitor-period=5s \
  --pod-eviction-timeout=2m0s \
  --controllers=*,bootstrapsigner,tokencleaner \
  --allocate-node-cidrs=true \
  --node-cidr-mask-size=24 \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=always
RestartSec=10s
[Install]
WantedBy=multi-user.target
HERE
```

#### 5.5.  启动 kube-controller-manager
```shell
# 分发启动文件到Master
for node in node1 node2 node3;do scp -r /root/k8s/master/systemd/kube-controller-manager.service $node:/lib/systemd/system/;done

# 启动 kube-controller-manager
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-controller-manager --now && systemctl status kube-controller-manager";done
```

### 6. 部署 kube-scheduler
#### 6.1.创建csr请求文件
```shell
cat > /root/k8s/ssl/kube-scheduler-csr.json  <<'HERE'
{
  "CN": "system:kube-scheduler",
  "hosts": [
    "127.0.0.1",
    "192.168.200.7",
    "192.168.200.8",
    "192.168.200.9"
  ],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen",
      "O": "system:kube-scheduler",
      "OU": "System"
    }
  ]
}
HERE
```

#### 6.2. 生成 kube-scheduler 证书
```
# 生成证书
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes kube-scheduler-csr.json | cfssljson --bare kube-scheduler

# 分发到 Master
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-scheduler*.pem $node:/etc/kubernetes/pki/;done
```
#### 6.3. 创建 kube-scheduler 的kubeconfig
```
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-scheduler@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-scheduler@kubernetes \
  --kubeconfig=kube-scheduler.kubeconfig


# 复制到 /etc/kubernetes
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-scheduler.kubeconfig $node:/etc/kubernetes/;done
```

#### 6.4. kube-scheduler启动文件
```shell
cat > /root/k8s/master/systemd/kube-scheduler.service <<'HERE'
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --v=2 \
  --logtostderr=true \
  --address=127.0.0.1 \
  --leader-elect=true \
  --client-ca-file=/etc/kubernetes/pki/ca.pem \
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig
  
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
HERE
```

#### 6.5.  启动 kube-scheduler
```
# 分发启动文件到Master
for node in node1 node2 node3;do scp -r /root/k8s/master/systemd/kube-scheduler.service $node:/lib/systemd/system/;done

# 启动 kube-scheduler
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-scheduler --now && systemctl status kube-scheduler";done
```

### 7. 部署 kubelet

#### 7.1. 创建 kubelet-bootstrap.kubeconfig
```shell
BOOTSTRAP_TOKEN=$(awk -F "," '{print $1}' /etc/kubernetes/ssl/token.csv)

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default \
  --kubeconfig=kubelet-bootstrap.kubeconfig
  
# 创建角色绑定
kubectl create clusterrolebinding kubelet-bootstrap \
  --clusterrole=system:node-bootstrapper \
  --user=kubelet-bootstrap
```

#### 7.2. kubelet 配置文件
>"cgroupDriver": "systemd"  如果docker的驱动为cgroupfs，处修改为cgroupfs。此处设置很重要，否则后面node节点无法加入到集群 https://blog.csdn.net/Andriy_dangli/article/details/85062983
```shell
cat > /root/k8s/node/systemd/kubelet.yaml <<'HERE'
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  x509:
    clientCAFile: "/etc/kubernetes/pki/ca.pem"
  webhook:
    enabled: true
    cacheTTL: 2m0s
  anonymous:
    enabled: false
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
address: 192.168.200.8
port: 10250
readOnlyPort: 10255
cgroupDriver: systemd
hairpinMode: promiscuous-bridge
serializeImagePulls: false
rotateCertificates: true
clusterDomain: cluster.local.
clusterDNS:
- 172.21.0.2
HERE
```

#### 7.3. kubelet 启动文件
```shell
cat > /root/k8s/node/systemd/kubelet.service <<'HERE'
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
  --cert-dir=/etc/kubernetes/pki \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --config=/etc/kubernetes/kubelet.yaml \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --pod-infra-container-image=k8s.gcr.io/pause:3.2 \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
HERE
```

#### 7.4. 分发配置文件和启动文件
```shell
# kubeconfig、kubelet.yaml、kubelet.service
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kubelet-bootstrap.kubeconfig $node:/etc/kubernetes/;done
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kubelet.yaml $node:/etc/kubernetes/;done
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kubelet.service $node:/lib/systemd/system/;done

# 其他节点修改/etc/kubernetes/kubelet.json 的IP
sed -i 's@192.168.200.8@192.168.200.9@' /etc/kubernetes/kubelet.yaml
sed -i 's@192.168.200.8@192.168.200.10@' /etc/kubernetes/kubelet.yaml
sed -i 's@192.168.200.8@192.168.200.11@' /etc/kubernetes/kubelet.yaml
sed -i 's@192.168.200.8@192.168.200.12@' /etc/kubernetes/kubelet.yaml
```

#### 7.5. 启动 kubelet
```shell
# 创建 kubelet 数据目录
for node in node1 node2 node3 node4 node5;do ssh $node "mkdir -p /var/lib/kubelet";done

# 启动
for node in node1 node2 node3 node4 node5;do ssh $node "systemctl daemon-reload && systemctl enable kubelet --now && systemctl status kubelet";done
```

#### 7.6. workder加入集群
>kubelet服务启动成功后，接着到master上Approve一下bootstrap请求。执行如下命令可以看到worker节点发送CSR 请求
```shell
kubectl get csr

NAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-9XZBBrGZ3ZcZjC9lqIdRl2Fn0gvhfKuAMeezRgXql8A   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-AXEZ65YFmt_vKVq8puhpZJgTMOi12Uzq0WUz7oTQ6ZE   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-J8uS6hmZYETrDwHeRHeBNtauyCOQx82WuxUbXkp7Sh0   52s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-XaPhQT-q14lOzgsRTm1Z6MWiKXRhnL-ByadXlithFs0   57s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-sK7uG0Jw7XRR4If2o3bdTswEofy9f0tAANSIxmQ2zLM   5m35s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending

# 通过证书签名请求
for i in $(kubectl get csr | awk 'NR!=1{print $0}' | awk '{print $1}');do kubectl certificate approve $i;done

# 查看节点
kubectl get nodes
NAME           STATUS     ROLES    AGE     VERSION
docker-k8s01   NotReady   <none>   2m31s   v1.20.1
docker-k8s02   NotReady   <none>   2m31s   v1.20.1
docker-k8s03   NotReady   <none>   2m30s   v1.20.1
docker-k8s04   NotReady   <none>   2m30s   v1.20.1
docker-k8s05   NotReady   <none>   2m31s   v1.20.1

```

### 8.  部署 kube-proxy
#### 8.1. kube-proxy-csr.json
```shell
cat >/root/k8s/ssl/kube-proxy-csr.json  <<'HERE'
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "ShenZhen",
      "O": "system:kube-proxy",
      "OU": "System"
    }
  ]
}
HERE
```
#### 8.2. 生成 kube-proxy 证书
```shell
# 生成证书
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes kube-proxy-csr.json | cfssljson --bare kube-proxy

# 分发到 Master
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kube-proxy*.pem $node:/etc/kubernetes/pki/;done
```
#### 8.3. 创建 kube-proxy 的kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-proxy@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-proxy@kubernetes \
  --kubeconfig=kube-proxy.kubeconfig
```

#### 8.4. kube-proxy配置文件
>clusterCIDR: 172.21.0.0/16                           # 此处网段必须与service网段保持一致，否则部署
```shell
cat > /root/k8s/node/systemd/kube-proxy.yaml <<'HERE'
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 192.168.200.8
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 172.21.0.0/16
healthzBindAddress: 192.168.200.8:10256
metricsBindAddress: 192.168.200.8:10249
mode: "ipvs"
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: "rr"
  strictARP: false
  syncPeriod: 30s
  tcpFinTimeout: 0s
  tcpTimeout: 0s
  udpTimeout: 0s
HERE
```

#### 8.5. kube-proxy 启动文件
```shell
cat > /root/k8s/node/systemd/kube-proxy.service <<'HERE'
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
HERE
```

#### 8.6. 分发配置文件和启动文件
```shell
# kubelet-proxy.kubeconfig、 kube-proxy.yaml、kube-scheduler.service
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kube-proxy.kubeconfig $node:/etc/kubernetes/;done
for node in node1 node2 node3 node4 node5;do scp -r  /root/k8s/node/systemd/kube-proxy.yaml $node:/etc/kubernetes/kube-proxy.yaml;done
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kube-proxy.service $node:/lib/systemd/system/;done

# 其他节点修改/etc/kuberneteskube-proxy.yaml 的IP
sed -i 's@192.168.200.8@192.168.200.9@g' /etc/kubernetes/kube-proxy.yaml
sed -i 's@192.168.200.8@192.168.200.10@g' /etc/kubernetes/kube-proxy.yaml
sed -i 's@192.168.200.8@192.168.200.11@g' /etc/kubernetes/kube-proxy.yaml
sed -i 's@192.168.200.8@192.168.200.12@g' /etc/kubernetes/kube-proxy.yaml
```

#### 8.7. 启动 kube-proxy
```shell
# 创建工作目录
for node in node1 node2 node3 node4 node5;do 
ssh $node "mkdir -p /var/lib/kube-proxy";done

# 启动
for node in node1 node2 node3 node4 node5;do ssh $node "systemctl daemon-reload && systemctl enable kube-proxy --now && systemctl status kube-proxy";done
```

### 9. 部署网络组件
>此时的状态节点都是NoReady，是因为网络组件没有配置
```shell
wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml

kubectl apply -f calico.yaml

kubectl get pods -A

# 此时再来看各节点的状态
kubectl get nodes
NAME           STATUS   ROLES    AGE   VERSION
docker-k8s01   Ready    <none>   67m   v1.20.1
docker-k8s02   Ready    <none>   67m   v1.20.1
docker-k8s03   Ready    <none>   67m   v1.20.1
docker-k8s04   Ready    <none>   67m   v1.20.1
docker-k8s05   Ready    <none>   67m   v1.20.1
```

### 10. 部署 Coredns
>kubernetes cluster.local in-addr.arpa ip6.arpa
forward . /etc/resolv.conf
clusterIP为：172.21.0.2（kubelet配置文件中的clusterDNS）
```shell
wget -O coredns.yaml https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed

kubernetes cluster.local in-addr.arpa ip6.arpa {
forward . /etc/resolv.conf {
clusterIP: 172.21.0.2

kubectl apply -f coredns.yaml
```

https://www.cnblogs.com/technology178/p/14295776.html
https://www.cnblogs.com/dinghc/p/13031436.html












