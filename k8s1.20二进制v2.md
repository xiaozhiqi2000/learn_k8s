[TOC]

# K8S 集群部署

[REFERENCE]<https://kubernetes.io/docs/home/>

## 环境介绍

节点部署
| 主机名 | IP | 配置 | Role | 备注 |
| ---- | ------ | ---- | ---- | ------- |
| docker-k8s01 | 192.168.200.8 | 2C4G | Master,Etcd | kube-apiserver、kube-controller-manager、kube-scheduler |
| docker-k8s02 | 192.168.200.9 | 2C4G | Master,Etcd | kube-apiserver、kube-controller-manager、kube-scheduler |
| docker-k8s03 | 192.168.200.10 | 2C4G | Master,Etcd | kubelet、kube-proxy |
| docker-k8s04 | 192.168.200.11 | 2C4G | Node,Etcd | kubelet、kube-proxy |
| docker-k8s05 | 192.168.200.12 | 2C4G | Node,Etcd | kubelet、kube-proxy |
| VIP | 192.168.200.88 |   |   | kube-apiserver VIP |

网络部署
|网络 | IP段 |
| ---- | ------ |
| 物理网络 | 192.168.200.0/24 |
| POD网络 | 10.244.0.0/16 |
| SERVICE网络 | 172.21.0.0/16 |


依赖组件
|  组件名称  |  组件版本  |
| ------------- | ----------- |
| Etcd |  |
| Docker | |
| Go | |
| CNI | |
| CSI | |
| Dashboard |  |
| Heapster | |
| Cluster | |
| kube-dns | |
| Influxdb | |
| Grafana | |
| Kibana | |
| cAdvisor | |
| Fluentd | |
| Elasticsearch | |
| go-oidc | |
| calico | |
| crictl | |
| CoreDNS | |
| event-exporter | |
| metrics-server | |
| ingress-gce | |
| ingress-nginx | |
| ip-masq-agent | |
| hcsshim | |

## 环境初始化（所有节点)
```shell
# 配置Yum源
rm -f /etc/yum.repos.d/* && wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo && wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo && yum clean all && yum makecache fast

# 升级软件包,Centos7需要升级，Centos8可以按需升级系统
yum update -y --exclude=kernel* && reboot

# 密钥互通node1分发
ssh-keygen -P "" -f /root/.ssh/id_rsa
cat /root/.ssh/id_rsa.pub > /root/.ssh/authorized_keys
for i in {2..5};do scp -r .ssh node$i:~/ ;done

# 主机名解析
cat > /etc/hosts <<EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.200.8 docker-k8s01 node1
192.168.200.9 docker-k8s02 node2
192.168.200.10 docker-k8s03 node3
192.168.200.11 docker-k8s04 node4
192.168.200.12 docker-k8s05 node5
EOF

# 关闭防火墙
systemctl disable firewalld;systemctl stop firewalld

# 禁用SELinux，允许容器访问宿主机的文件系统
setenforce 0
sed -i 's@SELINUX=.*@SELINUX=disabled@' /etc/selinux/config

# 关闭 you have new mail in
echo "unset MAILCHECK">> /etc/profile;source /etc/profile

# 同步时间
yum -y install chrony ntpdate
systemctl start chronyd && systemctl enable chronyd
ntpdate ntp1.aliyun.com

# 关闭SWAP
swapoff -a && sed -ri 's/.*swap.*/#&/' /etc/fstab
echo "vm.swappiness = 0">> /etc/sysctl.conf  && sysctl -p
```

## 升级内核（所有节点)
```shell
# 升级内核,安装长期支持版本，建议升级 4.18+ 不要太新
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm
yum --enablerepo=elrepo-kernel install kernel-lt -y

grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg

# 开启 User namespaces
grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"
grep user_namespace /boot/grub2/grub.cfg

# 查看版本
grubby --default-kernel

# 重启
reboot

# 查看内核版本
uname -a
Linux docker-k8s01 4.4.118-1.el7.elrepo.x86_64 #1 SMP Sun Feb 25 19:10:14 EST 2018 x86_64 x86_64 x86_64 GNU/Linux

# 安装 ipvs 软件包
yum install -y ipvsadm ipset sysstat contrack libseccomp net-tools

# 加载ipvs模块,在内核 4.19+版本nf_contrack_ipv4已经改为nf_contrack，4.18以下使用 nf_contrack_ipv4即可
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack

# 加入以下内容,在内核 4.19+版本nf_contrack_ipv4已经改为nf_contrack，4.18以下使用 nf_contrack_ipv4即可
cat > /etc/modules-load.d/ipvs.conf <<EOF
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
# 开启这个服务
systemctl enable --now systemd-modules-load.service

# 加载br_netfilter iptables 模块
cat <<EOF >  /etc/sysctl.d/k8s.conf
br_netfilter
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory = 1
vm.panic_on_oom = 0
fs.inotify.max_user_watchs = 89100
fs.file-max = 52706963
fs.nr_open = 52706963
net.netfilter.nf_conntrack_max = 2310720
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_sysncookies = 1
net.ipv4.tcp_max_sys_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_sys_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF

# 重新加载
sysctl --system

# 查看是否加载上ipvs模块
lsmod | grep -e ip_vs -e nf_conntrack

```

## 安装 Docker（所有节点)
[Docker官网安装文档](https://docs.docker.com/engine/install/)
[Docker镜像国内](https://kubernetes.feisky.xyz/appendix/mirrors)

```shell
# 下载阿里的docker源
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo

# 安装 docker 19.03.15
yum -y install docker-ce-19.03.15-3.el7 docker-ce-cli-19.03.15-3.el7 containerd.io-1.4.3-3.1.el7 

# 配置docker目录
mkdir -p /data/docker /etc/docker /etc/systemd/system/docker.service.d

# 配置国内镜像加速
cat > /etc/docker/daemon.json << EOF
{
  "graph": "/data/docker",
  "storage-driver": "overlay2",
  "registry-mirrors": [
      "https://0mmxqss3.mirror.aliyuncs.com",
      "https://docker.mirrors.ustc.edu.cn",
      "https://registry.docker-cn.com"
  ],
  "insecure-registries": ["registry.cn-hangzhou.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "live-restore": true,
  "log-driver": "json-file",
  "log-level": "warn",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
EOF

# 启动dcoker
systemctl start docker && systemctl enable docker --now && systemctl status docker
```

## 安装HA(master节点)

> haproxy + keepalived 也可以 nginx + keepalived 四层代理

### 1. haproxy

```shell
# 安装 haproxy 依赖
yum install -y haproxy keepalived libnl3-devel ipset-devel

# 配置 haproxy，注意修改成自己的IP
cat > /etc/haproxy/haproxy.cfg <<EOF
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /var/run/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master
        bind 0.0.0.0:8443
        mode tcp
        option tcplog
        balance source
        server s1 192.168.200.8:6443  check inter 10000 fall 2 rise 2 weight 1
        server s2 192.168.200.9:6443  check inter 10000 fall 2 rise 2 weight 1
        server s3 192.168.200.10:6443  check inter 10000 fall 2 rise 2 weight 1
EOF

# 分发其他节点
for node in node2 node3;do scp /etc/haproxy/haproxy.cfg $node:/etc/haproxy/;done

# 启动 haproxy
for node in node1 node2 node3;do ssh ${node} "systemctl start haproxy && systemctl enable haproxy --now";done

# 查看下服务情况
systemctl status haproxy
```

### 2. keepalived
```shell
# 配置 keepalived，注意修改成自己的IP和网卡名称
# master1 配置

cat > /etc/keepalived/keepalived.conf <<EOF
global_defs {
    script_user root 
    enable_script_security
    router_id lb-master
}

vrrp_script check-haproxy {
    script "/bin/bash -c 'if [[ $(netstat -nlp | grep 8443) ]]; then exit 0; else exit 1; fi'"
    interval 2
    weight -50
}

vrrp_instance VI-kube-master {
    state MASTER
    priority 120
    dont_track_primary
    interface ens33
    virtual_router_id 120
    advert_int 3
    track_script {
        check-haproxy
    }
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.200.88
    }
}

# notify "/container/service/keepalived/assets/notify.sh"
EOF

# 分发 keepalived
for node in node2 node3;do scp /etc/keepalived/keepalived.conf ${node}:/etc/keepalived/;done

# master2 修改
sed -i -e 's@state MASTER@state BACKUP@' -e 's@priority 120@priority 90@' /etc/keepalived/keepalived.conf

# master3 修改
sed -i -e 's@state MASTER@state BACKUP@' -e 's@priority 120@priority 100@' /etc/keepalived/keepalived.conf


# 启动 keepalived
for node in node1 node2 node3;do ssh ${node} "systemctl start keepalived && systemctl enable keepalived --now";done


# 查看下服务情况
systemctl status keepalived
```

## Etcd 集群部署（node01）

### 1. 安装 CFSSL 证书工具
```shell
wget -O /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 
wget -O /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget -O /usr/local/bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64

chmod +x  /usr/local/bin/cfssl*
```

### 2. 准备相关目录
```shell
mkdir -p /root/k8s/etcd/{ssl,bin,systemd}

# 创建etcd所需的目录
for node in node1 node2 node3;do ssh ${node} "mkdir -p /etc/etcd/ssl /var/lib/etcd";done
```

### 3. ca-config.json CA配置文件
```shell
cat > /root/k8s/etcd/ssl/ca-config.json  <<'HERE'
{
    "signing": {
        "default": {
            "expiry": "876000h"
        },
        "profiles": {
            "server": {
                "expiry": "876000h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth"
                ]
            },
            "client": {
                "expiry": "876000h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "876000h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
HERE
```

### 4. etcd-ca-csr.json CA 证书签名请求
```shell
cat > /root/k8s/etcd/ssl/etcd-ca-csr.json  <<'HERE'
{
  "CN": "etcd-ca",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "Shenzhen"
    }
  ],
  "ca": {
    "expiry": "876000h"
  }
}
HERE
```

### 5. etcd 证书签名请求
```shell
cat > /root/k8s/etcd/ssl/etcd-csr.json  <<'HERE'
{
    "CN": "etcd",
    "hosts": [
        "127.0.0.1",
        "192.168.200.8",
        "192.168.200.9",
        "192.168.200.10"
    ],
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [{
        "C": "CN",
        "ST": "GuangDong",
        "L": "Shenzhen"
    }]
}
HERE
```

### 6.etcd-peer证书csr请求文件

```shell
cat > /root/k8s/etcd/ssl/etcd-peer-csr.json <<EOF
{
    "CN": "etcd-peer",
    "hosts": [
        "127.0.0.1",
        "localhost",
        "192.168.200.8",
        "192.168.200.9",
        "192.168.200.10"
    ],
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [{
         "C": "CN",
         "ST": "GuangDong",
         "L": "ShenZhen"
    }]
}
EOF

```

### 7. 生成etcd证书

```shell
cd /root/k8s/etcd/ssl/
cfssl gencert --initca=true etcd-ca-csr.json | cfssljson --bare etcd-ca

cfssl gencert --ca etcd-ca.pem --ca-key etcd-ca-key.pem --config ca-config.json --profile peer etcd-csr.json | cfssljson --bare etcd

cfssl gencert --ca etcd-ca.pem --ca-key etcd-ca-key.pem --config ca-config.json --profile peer etcd-peer-csr.json | cfssljson --bare etcd-peer

for node in node1 node2 node3;do scp -r /root/k8s/etcd/ssl/etcd*.pem $node:/etc/etcd/ssl/;done
```

### 8.下载etcd并解压
```shell
cd
wget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz
tar xf etcd-v3.4.13-linux-amd64.tar.gz
cp -f /root/etcd-v3.4.13-linux-amd64/etcd* /root/k8s/etcd/bin/
```

### 9.创建etcd启动文件
```shell
cat > /root/k8s/etcd/systemd/etcd.service <<'HERE'
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos
[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
--name=etcd01 \
--trusted-ca-file=/etc/etcd/ssl/etcd-ca.pem \
--cert-file=/etc/etcd/ssl/etcd.pem \
--key-file=/etc/etcd/ssl/etcd-key.pem \
--peer-trusted-ca-file=/etc/etcd/ssl/etcd-ca.pem \
--peer-cert-file=/etc/etcd/ssl/etcd-peer.pem \
--peer-key-file=/etc/etcd/ssl/etcd-peer-key.pem \
--initial-advertise-peer-urls=https://192.168.200.8:2380 \
--listen-peer-urls=https://192.168.200.8:2380 \
--listen-client-urls=https://192.168.200.8:2379,http://127.0.0.1:2379 \
--advertise-client-urls=https://192.168.200.8:2379 \
--initial-cluster-token=etcd-cluster \
--initial-cluster=etcd01=https://192.168.200.8:2380,etcd02=https://192.168.200.9:2380,etcd03=https://192.168.200.10:2380 \
--initial-cluster-state=new \
--data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
HERE
```

### 10. 下发etcd二进制文件
```shell
# 分发etcd命令
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/bin/ ${node}:/usr/local/bin/;done

# 下发etcd所需的service文件
for node in node1 node2 node3;do rsync -avP /root/k8s/etcd/systemd/ ${node}:/lib/systemd/system/;done

#修改service相应节点IP(node2,node3节点执行)
# node2 操作
sed -i -e 's@192.168.200.8@192.168.200.9@g' -e 's@etcd01@etcd02@' -e 's@--initial-cluster=etcd02=https://192.168.200.9:2380@--initial-cluster=etcd01=https://192.168.200.8:2380@g' /lib/systemd/system/etcd.service

# node3 操作
sed -i -e 's@192.168.200.8@192.168.200.10@g' -e 's@etcd01@etcd03@' -e 's@--initial-cluster=etcd03=https://192.168.200.10:2380@--initial-cluster=etcd01=https://192.168.200.8:2380@g' /lib/systemd/system/etcd.service
```

### 11.启动etcd集群
```shell
# 启动etcd集群,由于集群需要保证quorum机制，因此至少有两个节点启动服务，集群初始化启动成功
systemctl daemon-reload && systemctl enable etcd --now
```

### 12.检查集群健康
```shell
# node1,node2,node3,随便1个节点执行
ETCDCTL_API=3 etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem\
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 endpoint health
 
ETCDCTL_API=3 etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem\
  --cert=/etc/etcd/ssl/etcd.pem \
  --key=/etc/etcd/ssl/etcd-key.pem \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 member list
  
ETCDCTL_API=3 etcdctl --write-out="table" \
  --cacert=/etc/etcd/ssl/etcd-ca.pem  \
  --cert=/etc/etcd/ssl/etcd.pem   \
  --key=/etc/etcd/ssl/etcd-key.pem   \
  --endpoints=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 endpoint status

```

## Kubernetes 组件部署
### 1. 准备CA证书

#### 1.1. 准备相关目录
```shell
# 创建ssl文件夹
mkdir -p /root/k8s/ssl/ && cd /root/k8s/ssl/

for node in node1 node2 node3 node4 node5;do ssh ${node} "mkdir -p /etc/kubernetes/pki/";done
```

#### 1.2. ca-config.json CA配置文件
```shell
# 和 etcd 的 ca-config.json 一样即可
cp /root/k8s/etcd/ssl/ca-config.json /root/k8s/ssl/ca-config.json
```

#### 1.3. ca-csr.json CA证书签名请求
```shell
cat > /root/k8s/ssl/ca-csr.json  <<'HERE'
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "Shenzhen",
      "O": "k8s",
      "OU": "System"
    }
  ],
  "ca": {
    "expiry": "876000h"
  }
}
HERE
```

#### 1.4. pront-proxy-ca-csr.json

```shell
cat > /root/k8s/ssl/front-proxy-ca-csr.json <<EOF
{
    "CN": "front-proxy-ca",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen"
        }
    ],
    "ca": {
        "expiry": "876000h"
    }
}
EOF
```

#### 1.5. 生成CA证书和CA key

```shell
cfssl gencert --initca=true ca-csr.json | cfssljson --bare ca
cfssl gencert --initca=true front-proxy-ca-csr.json | cfssljson --bare front-proxy-ca

# 分发到 Master 节点
for node in node1 node2 node3;do scp -r /root/k8s/ssl/ca*.pem /root/k8s/ssl/front-proxy-ca*.pem $node:/etc/kubernetes/pki/;done && cd
```

### 2. 下载kubernetes二进制并下发
```shell
# 目录准备
mkdir -p /root/k8s/{master,node}/{bin,systemd}

for node in node1 node2 node3 node4 node5;do ssh ${node} "mkdir -p /etc/kubernetes/manifests/ /var/lib/kubelet /var/log/kubernetes";done

# 下载k8s 1.20.1二进制包
wget https://dl.k8s.io/v1.20.1/kubernetes-server-linux-amd64.tar.gz

# 解压
tar xf kubernetes-server-linux-amd64.tar.gz && tar xf kubernetes/kubernetes-src.tar.gz -C kubernetes/

# 根据master，node进行归类
cp -f /root/kubernetes/server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /root/k8s/master/bin/

cp -f /root/kubernetes/server/bin/{kubelet,kube-proxy} /root/k8s/node/bin/

# 下发master二进制文件
for node in node1 node2 node3;do rsync -avP /root/k8s/master/bin/ ${node}:/usr/local/bin/;done

# 下发worker二进制文件
for node in node1 node2 node3 node4 node5;do rsync -avP /root/k8s/node/bin/ ${node}:/usr/local/bin/;done
```

### 3. 部署kube-apiserver 
>注意：此处需要将dns首ip、etcd、master、node、service、VIP的ip都填上

#### 3.1. kube-apiserver-csr.json
```shell
cat > /root/k8s/ssl/kube-apiserver-csr.json  <<'HERE'
{
  "CN": "kube-apiserver",
  "hosts": [
    "127.0.0.1",
    "192.168.200.8",
    "192.168.200.9",
    "192.168.200.10",
    "192.168.200.11",
    "192.168.200.12",
    "192.168.200.88",
    "172.21.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "Shenzhen",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
HERE
```

#### 3.2. kube-apiserver-etcd-client-csr.json

```shell
cat > /root/k8s/ssl/kube-apiserver-etcd-client-csr.json <<EOF
{
    "CN": "kube-apiserver-etcd-client",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen",
            "O": "system:masters"
        }
    ]
}
EOF
```

#### 3.3. front-proxy-client-csr.json

> api访问etcd的client证书

```shell
cat > /root/k8s/ssl/front-proxy-client-csr.json <<EOF
{
    "CN": "front-proxy-client",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "ShenZhen"
        }
    ]
}
EOF
```

#### 3.4. 生成证书

```shell
cd /root/k8s/ssl/

cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile server kube-apiserver-csr.json | cfssljson --bare kube-apiserver

cfssl gencert -ca /root/k8s/etcd/ssl/etcd-ca.pem -ca-key /root/k8s/etcd/ssl/etcd-ca-key.pem \
-config ca-config.json -profile client kube-apiserver-etcd-client-csr.json | cfssljson -bare kube-apiserver-etcd-client

cfssl gencert -ca front-proxy-ca.pem -ca-key front-proxy-ca-key.pem -config ca-config.json -profile client front-proxy-client-csr.json | cfssljson -bare front-proxy-client

# 分发证书
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-apiserver*.pem /root/k8s/ssl/kube-apiserver-etcd-client*.pem /root/k8s/ssl/front-proxy-client*.pem $node:/etc/kubernetes/pki/;done

for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-apiserver-etcd-client*.pem $node:/etc/etcd/ssl/;done
```

#### 3.5. 生成token文件(可选)

> 如果 TLS bootrap 自动颁发证书的话，这步可不做

```shell
cd /root/k8s/ssl

cat > /root/k8s/ssl/token.csv <<EOF
$(head -c 16 /dev/urandom | od -An -t x | tr -d ' '),kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF

for node in node1 node2 node3;do scp -r /root/k8s/ssl/token.csv $node:/etc/kubernetes/pki/;done
```

#### 3.6. kube-apiserver 启动文件

> 如果上述做了，需要指定 --token-auth-file=/etc/kubernetes/pki/token.csv

```shell
cat > /root/k8s/master/systemd/kube-apiserver.service  <<'HERE'
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --v=2 \
  --logtostderr=true \
  --bind-address=0.0.0.0 \
  --secure-port=6443 \
  --insecure-port=0 \
  --advertise-address=192.168.200.8 \
  --service-cluster-ip-range=172.21.0.0/16 \
  --service-node-port-range=30000-32767 \
  --etcd-servers=https://192.168.200.8:2379,https://192.168.200.9:2379,https://192.168.200.10:2379 \
  --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \
  --etcd-certfile=/etc/etcd/ssl/kube-apiserver-etcd-client.pem \
  --etcd-keyfile=/etc/etcd/ssl/kube-apiserver-etcd-client-key.pem \
  --client-ca-file=/etc/kubernetes/pki/ca.pem \
  --tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem \
  --tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem \
  --kubelet-client-certificate=/etc/kubernetes/pki/kube-apiserver.pem \
  --kubelet-client-key=/etc/kubernetes/pki/kube-apiserver-key.pem \
  --service-account-key-file=/etc/kubernetes/pki/ca-key.pem \
  --service-account-signing-key-file=/etc/kubernetes/pki/ca-key.pem  \
  --service-account-issuer=https://kubernetes.default.svc.cluster.local \
  --kubelet-preferred-address-types=InternalIP,ExternnalIP,Hostname \
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,ResourceQuota \
  --authorization-mode=RBAC,Node \
  --enable-bootstrap-token-auth=true \
  --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem
  --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem
  --requestheader-allowed-names=aggregator \
  --requestheader-group-headers=X-Remote-Group \
  --requestheader-extra-headers-prefix=X-Remote-Extra- \
  --requestheader-username-headers=X-Remote-User \
  --enable-swagger-ui=true
  #--token-auth-file=/etc/kubernetes/pki/token.csv

Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
HERE
```

#### 3.7. 分发 kube-apiserver 启动文件
```shell
for node in node1 node2 node3;do scp -r /root/k8s/master/systemd/kube-apiserver.service $node:/lib/systemd/system/;done

# node2修改IP
sed -i -e 's@192.168.200.8@192.168.200.9@g' -e 's@192.168.200.9:2379@192.168.200.8:2379@' /lib/systemd/system/kube-apiserver.service

# node3修改IP
sed -i -e 's@192.168.200.8@192.168.200.10@g' -e 's@192.168.200.10:2379@192.168.200.8:2379' /lib/systemd/system/kube-apiserver.service
```

#### 3.8. 启动 kube-apiserver
```shell
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-apiserver --now && systemctl status kube-apiserver";done

# 有返回则正常
curl --insecure https://192.168.200.8:6443/

```

### 4. 部署 kubectl

#### 4.1. kubernetes-admin-csr.json

```shell
cat > /root/k8s/ssl/admin-csr.json <<EOF
{
    "CN": "admin",
    "key": {
        "algo": "rsa",
        "size": 4096
    },
    "names": [
        {
            "C": "CN",
            "ST": "GuangDong",
            "L": "Shenzhen",
            "O": "system:masters",
            "OU": "System"
        }
    ]
}
EOF
```

#### 4.2. 生成 kubectl 证书
```shell
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client admin-csr.json | cfssljson --bare admin

# 分发证书
for node in node1 node2 node3;do scp -r /root/k8s/ssl/admin*.pem $node:/etc/kubernetes/pki/;done
```

#### 4.3. 生成admin kubeconfig
>生成集群管理员admin kubeconfig配置文件供kubectl调用，kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书
```shell
# 设置一个集群叫 kubernetes
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube.kubeconfig
 
# 设置一个用户叫admin
kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kube.kubeconfig
 
# 设置上下文参数，admin绑定kubernets集群
kubectl config set-context admin@kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kube.kubeconfig
 
# 设置默认上下文
kubectl config use-context admin@kubernetes \
    --kubeconfig=kube.kubeconfig

# 创建目录,存放admin kubeconfig
for node in node1 node2 node3;do ssh $node "mkdir -p /root/.kube";done

# 分发复制
for node in node1 node2 node3;do scp /root/k8s/ssl/kube.kubeconfig  $node:/root/.kube/config;done
```

#### 4.4.授权kubernetes证书访问kubelet api权限(这个可不做)
```shell
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
```

#### 4.5. 查看集群组件状态
>上面步骤完成后，kubectl就可以与kube-apiserver通信了
```
kubectl cluster-info
kubectl get componentstatuses                                        
kubectl get all --all-namespaces
```

#### 4.6. kubectl 命令补全
```
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
kubectl completion bash > ~/.kube/completion.bash.inc
source '/root/.kube/completion.bash.inc'
echo "source '/root/.kube/completion.bash.inc'" >> $HOME/.bash_profile
```

### 5. 部署 kube-controller-manager
#### 5.1. kube-controller-manager-csr.json
```shell
cat > /root/k8s/ssl/kube-controller-manager-csr.json  <<'HERE'
{
  "CN": "system:kube-controller-manager",
  "hosts": [
    "127.0.0.1",
    "192.168.200.8",
    "192.168.200.9",
    "192.168.200.10"
  ],
  "key": {
    "algo": "rsa",
    "size": 4096
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "Shenzhen",
      "O": "system:kube-controller-manager",
      "OU": "System"
    }
  ]
}
HERE
```
#### 5.2. 生成 kube-controller-manager 证书
```shell
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client kube-controller-manager-csr.json | cfssljson --bare kube-controller-manager

# 分发到Master
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-controller-manager*.pem $node:/etc/kubernetes/pki/;done
```
#### 5.3. 创建kube-controller-manager kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-controller-manager@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-controller-manager@kubernetes \
  --kubeconfig=kube-controller-manager.kubeconfig


# 分发复制/etc/kubernetes/
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-controller-manager.kubeconfig  $node:/etc/kubernetes/;done
```

#### 5.4. kube-controller-manager启动文件
```shell
cat > /root/k8s/master/systemd/kube-controller-manager.service <<'HERE'
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-controller-manager \
  --v=2 \
  --logtostderr=true \
  --bind-address=127.0.0.1 \
  --root-ca-file=/etc/kubernetes/pki/ca.pem \
  --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem \
  --client-ca-file=/etc/kubernetes/pki/ca.pem \
  --service-account-private-key-file=/etc/kubernetes/pki/ca-key.pem \
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
  --leader-elect=true \
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \
  --use-service-account-credentials=true \
  --node-monitor-grace-period=40s \
  --node-monitor-period=5s \
  --pod-eviction-timeout=2m0s \
  --controllers=*,bootstrapsigner,tokencleaner \
  --allocate-node-cidrs=true \
  --cluster-name=kubernetes \
  --service-cluster-ip-range=172.21.0.0/16 \
  --cluster-cidr=10.244.0.0/16 \
  --node-cidr-mask-size=24
Restart=always
RestartSec=10s
[Install]
WantedBy=multi-user.target
HERE
```

#### 5.5.  启动 kube-controller-manager
```shell
# 分发启动文件到Master
for node in node1 node2 node3;do scp -r /root/k8s/master/systemd/kube-controller-manager.service $node:/lib/systemd/system/;done

# 启动 kube-controller-manager
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-controller-manager --now && systemctl status kube-controller-manager";done
```

### 6. 部署 kube-scheduler
#### 6.1.创建csr请求文件
```shell
cat > /root/k8s/ssl/kube-scheduler-csr.json  <<'HERE'
{
  "CN": "system:kube-scheduler",
  "hosts": [
    "127.0.0.1",
    "192.168.200.7",
    "192.168.200.8",
    "192.168.200.9"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "Shenzhen",
      "O": "system:kube-scheduler",
      "OU": "System"
    }
  ]
}
HERE
```

#### 6.2. 生成 kube-scheduler 证书
```
# 生成证书
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile client kube-scheduler-csr.json | cfssljson --bare kube-scheduler

# 分发到 Master
for node in node1 node2 node3;do scp -r /root/k8s/ssl/kube-scheduler*.pem $node:/etc/kubernetes/pki/;done
```
#### 6.3. 创建 kube-scheduler 的kubeconfig
```
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-scheduler@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-scheduler@kubernetes \
  --kubeconfig=kube-scheduler.kubeconfig


# 复制到 /etc/kubernetes
for node in node1 node2 node3;do scp /root/k8s/ssl/kube-scheduler.kubeconfig $node:/etc/kubernetes/;done
```

#### 6.4. kube-scheduler启动文件
```shell
cat > /root/k8s/master/systemd/kube-scheduler.service <<'HERE'
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --v=2 \
  --logtostderr=true \
  --address=127.0.0.1 \
  --leader-elect=true \
  --client-ca-file=/etc/kubernetes/pki/ca.pem \
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig
  
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
HERE
```

#### 6.5.  启动 kube-scheduler
```
# 分发启动文件到Master
for node in node1 node2 node3;do scp -r /root/k8s/master/systemd/kube-scheduler.service $node:/lib/systemd/system/;done

# 启动 kube-scheduler
for node in node1 node2 node3;do 
ssh $node "systemctl daemon-reload && systemctl enable kube-scheduler --now && systemctl status kube-scheduler";done
```

### 7.TLS  Bootstraping 自动颁发证书

https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/

https://kubernetes.io/zh/docs/reference/access-authn-authz/bootstrap-tokens/

https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/

https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery.md

https://www.jianshu.com/p/b2f7340d40a7

```shell
# 允许kublet TLS bootstrap创建CSR请求
kubectl create clusterrolebinding create-csrs-for-bootstrapping \
  --clusterrole=system:node-bootstrapper \
  --group=system:bootstrappers

# 允许kube-controller-manager自动为system:bootstrappers组的用户颁发证书
kubectl create clusterrolebinding auto-approve-csrs-for-group \
  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \
  --group=system:bootstrappers

# 允许system:nodes组用户自动续期证书
kubectl create clusterrolebinding auto-approve-renewals-for-nodes \
  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \
  --group=system:nodes

# 创建bootstrap-token，在master节点上执行
cat > /root/k8s/ssl/create-bootstrap-token.sh <<EOF
#!/bin/bash
# 指定 apiserver 地址
KUBE_APISERVER="https://192.168.200.88:8443"

# 生成 Bootstrap Token
TOKEN_ID=$(cat /dev/urandom | tr -dc 0-9a-z | head -c 6)
TOKEN_SECRET=$(cat /dev/urandom | tr -dc 0-9a-z | head -c 16)
BOOTSTRAP_TOKEN="\${TOKEN_ID}.\${TOKEN_SECRET}"
echo "Bootstrap Token: \${BOOTSTRAP_TOKEN}"

# 使用secret存储token 
cat > /root/k8s/ssl/bootstrap-secret.yaml <<HERE
apiVersion: v1
kind: Secret
metadata:
  # 名字必须该格式
  name: bootstrap-token-\${TOKEN_ID}
  namespace: kube-system
# 必须该类型
type: bootstrap.kubernetes.io/token
stringData:
  description: "TLS引导令牌"
  token-id: \${TOKEN_ID}
  token-secret: \${TOKEN_SECRET}
  # expiration可选参数，设置token过期时间
  expiration: $(date --rfc-3339=seconds -d "6 hours" | sed 's/ /T/')
  usage-bootstrap-authentication: "true"
  usage-bootstrap-signing: "true"
  #auth-extra-groups: system:bootstrappers:default-node-token
HERE
kubectl apply -f /root/k8s/ssl/bootstrap-secret.yaml
EOF

bash /root/k8s/ssl/create-bootstrap-token.sh

```

### 8. 部署 kubelet

#### 8.1. 创建 kubelet-bootstrap.kubeconfig
```shell
cd /root/k8s/ssl/
# 生成 kubelet bootstrap-kubeconfig引导文件，注意：这里的${TOKEN_ID}和${BOOTSTRAP_TOKEN}改成上面TLS引导配置部分创建的值
TOKEN_ID=$(cat bootstrap-secret.yaml | sed -n '/token-id/s#.*: ##p')
BOOTSTRAP_TOKEN=$(cat bootstrap-secret.yaml | sed -n '/token-secret/s#.*: ##p')

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem \
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials "system:bootstrap:${TOKEN_ID}" \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
  --cluster=kubernetes \
  --user="system:bootstrap:${TOKEN_ID}" \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default \
  --kubeconfig=kubelet-bootstrap.kubeconfig
```

#### 8.2. kubelet 配置文件
>"cgroupDriver": "cgroupfs",                     # 如果docker的驱动为systemd，处修改为systemd。此处设置很重要，否则后面node节点无法加入到集群 https://blog.csdn.net/Andriy_dangli/article/details/85062983
```
cat > /root/k8s/node/systemd/kubelet.yaml <<'HERE'
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  x509:
    clientCAFile: "/etc/kubernetes/pki/ca.pem"
  webhook:
    enabled: true
    cacheTTL: 2m0s
  anonymous:
    enabled: false
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
address: 192.168.200.8
port: 10250
readOnlyPort: 10255
cgroupDriver: systemd
hairpinMode: promiscuous-bridge
serializeImagePulls: false
rotateCertificates: true
clusterDomain: cluster.local.
clusterDNS:
- 172.21.0.2
HERE
```

#### 8.3. kubelet 启动文件
```shell
cat > /root/k8s/node/systemd/kubelet.service <<'HERE'
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service
[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
  --cert-dir=/etc/kubernetes/pki \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --config=/etc/kubernetes/kubelet.yaml \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --pod-infra-container-image=k8s.gcr.io/pause:3.2 \
  --logtostderr=true \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
HERE
```

#### 8.4. 分发配置文件和启动文件
```shell
# kubelet-bootstrap.kubeconfig
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kubelet-bootstrap.kubeconfig $node:/etc/kubernetes/;done

# kubelet.yaml
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kubelet.yaml $node:/etc/kubernetes/;done

# kubelet.service
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kubelet.service $node:/lib/systemd/system/;done

# 其他节点修改/etc/kubernetes/kubelet.json 的IP
sed -i 's@192.168.200.8@192.168.200.9@' /etc/kubernetes/kubelet.yaml
sed -i 's@192.168.200.8@192.168.200.10@' /etc/kubernetes/kubelet.yaml
sed -i 's@192.168.200.8@192.168.200.11@' /etc/kubernetes/kubelet.yaml
sed -i 's@192.168.200.8@192.168.200.12@' /etc/kubernetes/kubelet.yaml
```

#### 8.5. 启动 kubelet
```shell
# 创建 kubelet 数据目录
for node in node1 node2 node3 node4 node5;do ssh $node "mkdir -p /var/lib/kubelet";done

# 启动
for node in node1 node2 node3 node4 node5;do ssh $node "systemctl daemon-reload && systemctl enable kubelet --now && systemctl status kubelet";done
```

#### 8.6. workder加入集群
>kubelet服务启动成功后，接着到master上Approve一下bootstrap请求。执行如下命令可以看到worker节点发送CSR 请求
```
kubectl get csr

NAME                                                   AGE     SIGNERNAME                                    REQUESTOR           CONDITION
node-csr-9XZBBrGZ3ZcZjC9lqIdRl2Fn0gvhfKuAMeezRgXql8A   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-AXEZ65YFmt_vKVq8puhpZJgTMOi12Uzq0WUz7oTQ6ZE   5m36s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-J8uS6hmZYETrDwHeRHeBNtauyCOQx82WuxUbXkp7Sh0   52s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-XaPhQT-q14lOzgsRTm1Z6MWiKXRhnL-ByadXlithFs0   57s     kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending
node-csr-sK7uG0Jw7XRR4If2o3bdTswEofy9f0tAANSIxmQ2zLM   5m35s   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending

# 通过证书签名请求
for i in $(kubectl get csr | awk 'NR!=1{print $0}' | awk '{print $1}');do kubectl certificate approve $i;done


# 查看节点
kubectl get nodes
NAME           STATUS     ROLES    AGE     VERSION
docker-k8s01   NotReady   <none>   2m31s   v1.20.1
docker-k8s02   NotReady   <none>   2m31s   v1.20.1
docker-k8s03   NotReady   <none>   2m30s   v1.20.1
docker-k8s04   NotReady   <none>   2m30s   v1.20.1
docker-k8s05   NotReady   <none>   2m31s   v1.20.1

```

### 9.  部署 kube-proxy
#### 9.1. kube-proxy-csr.json
```shell
cat >/root/k8s/ssl/kube-proxy-csr.json  <<'HERE'
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "GuangDong",
      "L": "Shenzhen",
      "O": "system:kube-proxy",
      "OU": "System"
    }
  ]
}
HERE
```
#### 9.2. 生成 kube-proxy 证书
```shell
# 生成证书
cd /root/k8s/ssl/ && cfssl gencert --ca ca.pem --ca-key ca-key.pem --config ca-config.json --profile kubernetes kube-proxy-csr.json | cfssljson --bare kube-proxy

# 分发到 Master
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kube-proxy*.pem $node:/etc/kubernetes/pki/;done
```
#### 9.3. 创建 kube-proxy 的kubeconfig
```shell
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=ca.pem\
  --embed-certs=true \
  --server=https://192.168.200.88:8443 \
  --kubeconfig=kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
  
# 设置上下文参数
kubectl config set-context system:kube-proxy@kubernetes \
  --cluster=kubernetes \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-proxy@kubernetes \
  --kubeconfig=kube-proxy.kubeconfig
```

#### 9.4. kube-proxy配置文件
>clusterCIDR: 172.21.0.0/16                           # 此处网段必须与service网段保持一致，否则部署
```shell
cat > /root/k8s/node/systemd/kube-proxy.yaml <<'HERE'
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 192.168.200.8
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 172.21.0.0/16
healthzBindAddress: 192.168.200.8:10256
metricsBindAddress: 192.168.200.8:10249
mode: "ipvs"
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: "rr"
  strictARP: false
  syncPeriod: 30s
  tcpFinTimeout: 0s
  tcpTimeout: 0s
  udpTimeout: 0s
HERE
```

#### 9.5. kube-proxy 启动文件
```shell
cat > /root/k8s/node/systemd/kube-proxy.service <<'HERE'
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --config=/etc/kubernetes/kube-proxy.yaml \
  --logtostderr=true
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
HERE
```

#### 9.6. 分发配置文件和启动文件
```shell
# kubelet-proxy.kubeconfig
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/ssl/kube-proxy.kubeconfig $node:/etc/kubernetes/;done

# kube-proxy.yaml
for node in node1 node2 node3 node4 node5;do scp -r  /root/k8s/node/systemd/kube-proxy.yaml $node:/etc/kubernetes/kube-proxy.yaml;done

# kube-scheduler.service
for node in node1 node2 node3 node4 node5;do scp -r /root/k8s/node/systemd/kube-proxy.service $node:/lib/systemd/system/;done

# 其他节点修改/etc/kuberneteskube-proxy.yaml 的IP
sed -i 's@192.168.200.8@192.168.200.9@g' /etc/kubernetes/kube-proxy.yaml
sed -i 's@192.168.200.8@192.168.200.10@g' /etc/kubernetes/kube-proxy.yaml
sed -i 's@192.168.200.8@192.168.200.11@g' /etc/kubernetes/kube-proxy.yaml
sed -i 's@192.168.200.8@192.168.200.12@g' /etc/kubernetes/kube-proxy.yaml
```

#### 9.7. 启动 kube-proxy
```shell
# 创建工作目录
for node in node1 node2 node3 node4 node5;do 
ssh $node "mkdir -p /var/lib/kube-proxy";done

# 启动
for node in node1 node2 node3 node4 node5;do ssh $node "systemctl daemon-reload && systemctl enable kube-proxy --now && systemctl status kube-proxy";done
```

### 10. 部署网络组件
>此时的状态节点都是NoReady，是因为网络组件没有配置
```
wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml
kubectl apply -f calico.yaml

kubectl get pods -A

# 此时再来看各节点的状态
kubectl get nodes
NAME           STATUS   ROLES    AGE   VERSION
docker-k8s01   Ready    <none>   67m   v1.20.1
docker-k8s02   Ready    <none>   67m   v1.20.1
docker-k8s03   Ready    <none>   67m   v1.20.1
docker-k8s04   Ready    <none>   67m   v1.20.1
docker-k8s05   Ready    <none>   67m   v1.20.1

```

### 11. 部署 Coredns
>kubernetes cluster.local in-addr.arpa ip6.arpa
forward . /etc/resolv.conf
clusterIP为：172.21.0.2（kubelet配置文件中的clusterDNS）
```
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed

mv coredns.yaml.sed coredns.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local  in-addr.arpa ip6.arpa {
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. Default is 1.
  # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      affinity:
         podAntiAffinity:
           preferredDuringSchedulingIgnoredDuringExecution:
           - weight: 100
             podAffinityTerm:
               labelSelector:
                 matchExpressions:
                   - key: k8s-app
                     operator: In
                     values: ["kube-dns"]
               topologyKey: kubernetes.io/hostname
      containers:
      - name: coredns
        image: coredns/coredns:1.8.0
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 172.21.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP



kubectl apply -f coredns.yaml
```

https://www.cnblogs.com/technology178/p/14295776.html
https://www.cnblogs.com/dinghc/p/13031436.html












